\documentclass[11pt]{article}
\usepackage{listings,amsmath,amssymb,bbm}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{color}
\usepackage{multirow}
\usepackage{natbib}
\usepackage{algorithm}
\usepackage{algorithmic}
\oddsidemargin0cm
\topmargin-1.4cm
\textheight23.5cm
\textwidth16cm
%\parindent0cm
\renewcommand{\baselinestretch}{1.1}
\numberwithin{equation}{section}
\lstset{language=R,basicstyle=\ttfamily\footnotesize,breaklines=true}
\usepackage{booktabs}
\def\R{{\mathbb R}}  %%
\def\N{{\mathbb N}}  %%
\def\E{{\mathbb E}}  %%
\def\Z{{\mathbb Z}}  %%
\def\bc{\boldsymbol{c}}
\def\bp{\boldsymbol{p}}
\def\bd{\boldsymbol{d}}
\def\bx{\boldsymbol{x}}
\def\bm{\boldsymbol{m}}
\def\by{\boldsymbol{y}}
\def\bp{\boldsymbol{p}}
\def\bmu{\boldsymbol{\mu}}
\def\bphi{\boldsymbol{\phi}}
\def\bZ{\boldsymbol{Z}}
\def\bz{\boldsymbol{z}}

\newcounter{saveenumi}
\newcommand{\seti}{\setcounter{saveenumi}{\value{enumi}}}
\newcommand{\conti}{\setcounter{enumi}{\value{saveenumi}}}

\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\usepackage{url}%For ref url
%\bibliographystyle{plain}
%opening
\title{Insurance Loss modeling with Gradient Tree-Boosted Mixture Models}
\author{ Yanxi Hou\footnote{School of Data Science, Fudan University, 200433 Shanghai, China.} \and Jiahong Li\footnote{School of Mathematical Sciences, Peking University, 100871 Beijing, China.} \and Guangyuan Gao\footnote{Center for Applied Statistics and School of Statistics, Renmin University of China, 100872 Beijing, China.}}


\begin{document}

\maketitle

\begin{abstract}
%Insurance loss data always cannot be well modeled by a single distribution.

In actuarial practice, the mixture model is one widely applied statistical method to model the insurance loss data. 
Although the Expectation-Maximization (EM) algorithm usually plays an essential tool for the parameter estimation of mixture models, it suffers from other issues which cause unstable predictions. 
For example, feature engineering and variable selection are two crucial modeling issues that are challenging for mixture models as they involve several component models. 
Moreover, avoiding overfitting is another technical concern of the modeling method for the prediction of future losses. 
To address those issues, we propose an Expectation-Boosting (EB) algorithm, 
which replaces the maximization step in the EM algorithm by gradient boosting machines with regression trees. 
Our proposed EB algorithm can estimate the mixing probabilities and the component regression functions non-parametrically and overfitting-sensitively and
perform automated feature engineering, model fitting, and variable selection simultaneously, which
fully explores the predictive power of feature space. Moreover, the proposed algorithm can be combined with parallel computation methods to improve computation efficiency.
Finally, we conduct two simulation studies to show the good performance of the proposed algorithm and an empirical analysis of the claims amounts data for illustration. 
\end{abstract}

{\bf Keywords:} Insurance loss; Mixture models; EM algorithm; Gradient boosting; Parallel computation; Finite mixture of regressions; Zero-inflated Poisson model. 

\newpage
\section{Introduction}

It is well known that insurance loss data sometimes cannot be well modeled by a single distribution due to its complex data structures. 
For instance, Poisson distribution may not fit the claim count data well because of an excess of zero claims.
Claim amount data usually exhibit multimodal or heavy-tailed phenomenons, so a gamma distribution is insufficient to address these data features.
In statistical analysis, one alternative way is to apply a mixture of distributions for univariate loss data.
When the individual risk factors are available, a mixture of regressions is then a natural extension to modeling the risk heterogeneity, which is captured by the individual risk factors.
In the literature,
the mixture model is proposed by \citet{goldfeld1973markov} which serves as the basis of a Markov-switching regression model.
We refer to \citet{lindsay1995mixture} and \citet{peel2000finite} for a detailed review of mixture models.
In the field of machine learning research, a mixture model is often called a mixture of experts model,
which is an ensemble learning technique that implements the idea of training experts on subtasks of a predictive modeling problem \citep{jacobs1991adaptive,jiang1999hierarchical}.

Some recent studies in modeling insurance loss also focus on applying mixture models to deal with complicated data structures.
\citet{zhang2020type, zhang2022new} developed a multivariate zero-inflated hurdle model to describe multivariate count data with extra zeros.
\citet{delong2021gamma} proposed a mixture of neural networks with gamma loss to model insurance claim amounts.
\citet{lee2010modeling} and \citet{verbelen2015fitting} used a mixture of Erlangs to model insurance claim amounts including censored and truncated data.
\citet{lee2012modeling} developed the multivariate version of a mixture of Erlangs.
\citet{fung2019class2}, \citet{fung2019class} and \citet{tseung2021lrmoe} proposed a class of so called logit-weighted reduced mixture of experts (LRMoE) models for multivariate claim frequencies or severities distributions.
To the best of our knowledge, the existing studies always impose a linearity constraint on the regression function, i.e., under a generalized linear model (GLM) framework.
In this paper, we will relax this constraint by estimating regression functions non-parametrically.

Several aspects of modeling are very noteworthy in research. First, it is challenging to estimate parameters in mixture models since the parameters in component models are related to each other.
To overcome it,  \citet{dempster1977maximum} proposed the Expectation-Maximization (EM) algorithm, which is an iterative method to estimate the component parameters and the conditional expectation of latent component indicator variables. 
Although the EM algorithm becomes a standard approach for estimation of mixture models, it is hard to address other practical issues, for example, the feature engineering and the variable selection, both of which are also challenging since the dependence between the covariates and response variables varies from one component to another.  
\citet{khalili2007variable} introduced a penalized likelihood approach for the variable selection.
\citet{huang2012mixture} and \citet{huang2013nonparametric} proposed a non-parametric mixture of regressions to relax the linearity assumption on the regression function.
Moreover, selecting the number of the components is another challenging problem.
\citet{naik2007extending} derived a new information criterion for the selection of the component number.
\citet{kasahara2015testing} proposed a likelihood-based test to contrast mixture models with different numbers of components. 
In this paper, we assume that the number of components is known, and we will address  feature engineering and variable selection in the proposed method.

We propose an Expectation-Boosting (EB) algorithm for mixture models, which replaces the maximization step in the EM algorithm with a boosting step. 
It is well known that the boosting algorithm is an iterative method to estimate the regression function non-parametrically, where weak learners are calibrated step-wisely and added together.
Commonly used boosting algorithms include 
%We roughly categorize some commonly used boosting algorithms into three groups: 
%(a) Binary classification: 
AdaBoost, LogitBoost (real AdaBoost),
%(real, discrete, gentle AdaBoost), AdaBoost.M1; 
%(b) Multinomial classification: 
stagewise additive modeling using a multi-class exponential loss (SAMME), multi-class real AdaBoost, gradient boosting machine (GBM), etc. 
A class of boosting algorithms is based on gradient of loss functions including {gradient boosting machine (GBM)}, Newton boosting, gradient boosting decision tree (GBDT), {eXtreme Gradient Boosting (XGBoost)}, {light gradient boosting machine (LightGBM)}, etc.
%The last type is based on {gradient of loss function} and 
\citet{friedman2001greedy} has studied the gradient based boosting from a statistical perspective as an additive model.

Our EB algorithm follows the GBDT algorithm in the boosting step.
With the negative log-likelihood function as the loss function for boosting, the boosting step increases the likelihood at each iteration of the EB algorithm. 
There are several advantages of the EB algorithm over the EM algorithm. 
First, the boosting algorithm is a flexible non-parametric regression method, which facilitates both non-linear effects and interaction of covariates. 
The feature engineering and variable selection are performed automatically during the model fitting. 
Second, the boosting algorithm is overfitting-sensitive given suitable regularization parameters \citep{buehlmann:2003}. This property is beneficial since the ultimate goal of insurance loss modeling is to predict future losses. Therefore,
the out-of-sample prediction is more important than the goodness-of-fitting in our applications. 

%Boosting algorithm has a close relationship with the additive model.

The rest of the paper is structured as follows. 
Section \ref{sec:review} reviews mixture models and the EM algorithm. 
Section \ref{sec:EB} presents the EB algorithm. We also discuss interpretation, hyperparameters tuning, initialization and parallel computation in this section.   
Section \ref{sec:application} studies two simulated data and one real data to illustrate the proposed methodology. 
Section \ref{sec:conclusions} concludes the paper with several important findings.  The R code of our work is publicly available on the github (\url{https://github.com/sxpyggy/boosting-mixture-code}).


\section{Mixture models and the EM algorithm revisited}\label{sec:review}

In this section, we first review the mixture model, particularly a finite mixture of regressions \citep{peel2000finite}, and discuss different mixture structures. 
Then we review the EM algorithm, which is the foundation of the proposed EB algorithm. 
In this paper, we focus on the exponential distribution family with an expectation parameter $\mu$ and a dispersion parameter $\phi$. 
The exponential distributions form a general family of parametric distributions, which fit well different types of insurance loss data.

\subsection{Mixture models}\label{review:mix1}
Suppose a random variable $Y$ follows a finite mixture of distributions with the $k$-th {\it component distribution} from a finite collection of $K\ge2$ distributions
	$$\{f_1(Y;\mu_1,\phi_1),\ldots,f_K(Y;\mu_K,\phi_K)\}.$$
The mixing probabilities $\bp=(p_1,\ldots,p_K)$ lie in the $(K-1)$-unit simplex $\mathcal{P}\subset(0,1)^K$, i.e. have normalization $\sum_{k=1}^Kp_k=1$.
	The probability density function for $Y$ is given by a weighted sum of component distributions such that
	$$f(Y)=\sum_{k=1}^Kp_kf_k(Y;\mu_k,\phi_k).$$
	If the $J$-dimensional {\it individual feature} vector $\bx\in\mathcal{X}\subset\R^J$ is available and has a systematic effect on the response variable $Y$, then we can establish a finite mixture of regressions:
	\begin{equation}\label{mix-gen}
		f(Y|\bx)=\sum_{k=1}^Kp_k(\bx)f_k(Y|\bx;\mu_k(\bx),\phi_k(\bx)),
	\end{equation}
where we assume the parametric models of $p:\mathcal{X}\rightarrow\mathcal{P},\mu_k:\mathcal{X}\rightarrow\R,\phi_k: \mathcal{X}\rightarrow\R_+$ such that
\begin{align*}
	p(\bx)&=(p_1(\bx),\ldots,p_K(\bx))=(p_1(\bx;\theta_p),\ldots,p_K(\bx;\theta_p))\in\mathcal{P},\\
	\mu_k(\bx)&=\mu_k(\bx;\theta_\mu^{(k)})\in\R,\text{~~for~~}k=1,\ldots,K,\\
	\phi_k(\bx)&=\phi_k(\bx;\theta_\phi^{(k)})\in\R_+,\text{~~for~~}k=1,\ldots,K.
\end{align*}
Here $\theta_p, (\theta_\mu^{(k)})_{k=1:K}, (\theta_\phi^{(k)})_{k=1:K}$ are the unknown parameters.
Note that $\theta_p$ does not have a superscript since it corresponds to the coefficients in a multinomial logistic classification while $\theta_\mu^{(k)},\theta_\phi^{(k)}$ can be considered in separate component models; see Section \ref{sec:EM}.

	Equation \eqref{mix-gen} is a general representation for a finite mixture of regressions, 
	i.e., both mixing probabilities and component parameters depend on $\bx$.
	In real applications,  based on the features of data and the goal of modeling, we can make specific assumptions on \eqref{mix-gen}. For example,
	\begin{itemize}
		\item The mixing probabilities are not related to $\bx$
		\begin{equation*}
			f(Y|\bx)=\sum_{k=1}^Kp_kf_k(Y|\bx;\mu_k(\bx),\phi_k(\bx));
		\end{equation*}
		\item Both mixing probabilities and dispersions are not related to $\bx$
		\begin{equation*}
f(Y|\bx)=\sum_{k=1}^Kp_kf_k(Y|\bx;\mu_k(\bx),\phi_k);
		\end{equation*}

		\item If the component distributions are from the same distribution family $f$, we might assume a single dispersion $\phi$ 
\begin{equation}\label{gen-disp}
f(Y|\bx)=\sum_{k=1}^Kp_k(\bx)f(Y|\bx;\mu_k(\bx),\phi);
\end{equation}
		\item The feature vector $\bx$ is only related to the mixing probabilities
\begin{equation}\label{mix-lrmoe}
	f(Y|\bx)=\sum_{k=1}^Kp_k(\bx)f_k(Y;\mu_k,\phi_k).
\end{equation}
	\end{itemize}
	Selection among the above model alternatives can be viewed as imposing suitable constraints on the general form \eqref{mix-gen}, 
	which accelerates and stabilizes model fitting without compromising predictive performance. 
	The LRMoE proposed by \citet{fung2019class} uses \eqref{mix-lrmoe}.
	In Section \ref{sec:application}, the first simulated data follows \eqref{mix-gen} with $K=2$. The second simulated data follows \eqref{gen-disp} with $K=3$. The real data example uses \eqref{mix-lrmoe} with $K=5$.
	%
	%It provides practical flexibility according to different data structures and goals of modeling.
	% However, the proposed EB algorithm can still {accelerate and stabilize model fitting} without compromising predictive performance.

\subsection{The EM algorithm}\label{sec:EM}

	Suppose that for a single sample, we know exactly which component distribution it comes from, that is, we know the \textit{full information} $(Y,\bZ,\bx)$, where
	$$\bZ=(Z_1,\ldots,Z_K)=(\mathbbm{1}_1(k),\ldots,\mathbbm{1}_K(k))$$
	is the one-hot encoding vector of \textit{component indicator variable $k$}.
	Then the joint distribution function of $(Y,\bZ)$ given $\bx$ is
	$$f(Y,\bZ|
	\bx) = \prod_{k=1}^K\left[p_k(\bx)f_k(Y|\bx;\mu_k(\bx),\phi_k(\bx))\right]^{Z_k},$$
	or equivalently, the {complete log-likelihood function} is 
	\begin{equation}\label{full-L}
		%l(p,\mu,\phi|y,\bz,\bx)=
		\sum_{k=1}^K Z_k\left[\log p_k(\bx) + \log f_k(Y|\bx;\mu_k(\bx),\phi_k(\bx))\right].
	\end{equation}

	 
	 Given a data set $(y_i,\bz_i,\bx_i)_{i=1:n}$, 
	 The regression coefficients $\theta_p, (\theta_\mu^{(k)})_{k=1:K}, (\theta_\phi^{(k)})_{k=1:K}$ can be estimated by the following $K+1$ {independent optimizations:}
	\begin{equation}\label{p-reg}
		\hat{\theta}_p=\underset{\theta_p}{\arg\max}\sum_{i=1}^n\sum_{k=1}^Kz_{i,k}\log p_k(\bx_i;\theta_p)
	\end{equation}
and
	\begin{equation}\label{comp-reg}
		\left(\hat{\theta}_\mu^{(k)},\hat{\theta}_\phi^{(k)}\right)=\underset{\theta^{(k)}_\mu,\theta^{(k)}_\phi}{\arg\max}\sum_{i=1}^n\sum_{k=1}^Kz_{i,k}\log f_k\left(y_i;\mu_k\left(\bx_i;\theta_\mu^{(k)}\right),\phi_k\left(\bx_i;\theta_\phi^{(k)}\right)\right), ~\text{for} ~ k=1,\ldots,K.
	\end{equation}
	The optimizations \eqref{p-reg} and \eqref{comp-reg} are corresponding to  {\it a multinomial logistic classification} and {\it $K$ regressions}, respectively.
The multinomial logistic classification \eqref{p-reg} is fitted to all samples, while the $k$-th regression in \eqref{comp-reg} is fitted to {partial} samples with $\{i:z_{i,k}=1\}$.
In practice we do not have the full information  $(Y,\bZ,\bx)$, but only the incomplete information $(Y,\bx)$. The EM algorithm is inspired by the above discussion.
\paragraph{Expectation step.}
	With iterated parameters $\hat{\theta}_p, (\hat{\theta}_\mu^{(k)})_{k=1:K}, (\hat{\theta}_\phi^{(k)})_{k=1:K}$, calculate the \textit{conditional expectation} of $\bz$:
	$$\hat{z}_{i,k}=\hat{z}_k(\bx_i)=\frac{\hat{p}_{i,k}f_k(y_i;\hat{\mu}_{i,k},\hat{\phi}_{i,k})}{\sum_{l=1}^K\hat{p}_{i,l}f_l(y_i;\hat{\mu}_{i,l},\hat{\phi}_{i,l})},$$
	where
	$\hat{p}_{i,k}=\hat{p}_k(\bx_i)= p_k(\bx_i;\hat{\theta}_p),\hat{\mu}_{i,k}=\hat{\mu}_k(\bx_i)=\mu_k\left(\bx_i;\hat{\theta}_\mu^{(k)}\right),\hat{\phi}_{i,k}=\hat{\phi}_k(\bx_i)=\phi_k\left(\bx_i;\hat{\theta}_\phi^{(k)}\right)$.

\paragraph{Maximization step.}
	Based on the log-likelihood function for full information $(y_i,\hat{\bz}_i,\bx_i)_{i=1:n}$
	\begin{equation}\label{likelihood}
		\begin{aligned}
			&l\left(\theta_p, (\theta_\mu^{(k)})_{k=1:K}, (\theta_\phi^{(k)})_{k=1:K}\right)\\
			=&\sum_{i=1}^n\sum_{k=1}^K \hat{z}_{i,k}\left[\log p_{i,k} + \log f_k\left(y_i;\mu_{i,k},\phi_{i,k}\right)\right]\\
			=&\sum_{i=1}^n\sum_{k=1}^K \hat{z}_{i,k}\log p_k(\bx_i;\theta_p) + \sum_{i=1}^n\sum_{k=1}^K \hat{z}_{i,k}\log f_k\left(y_i;\mu_k\left(\bx_i;\theta_\mu^{(k)}\right),\phi_k\left(\bx_i;\theta_\phi^{(k)}\right)\right),
		\end{aligned}
	\end{equation}
 the MLEs of parameters $\theta_p, (\theta_\mu^{(k)})_{k=1:K}, (\theta_\phi^{(k)})_{k=1:K}$ are given by the following $K+1$ independent optimizations
 	\begin{equation}\label{p-reg2}
 	\hat{\theta}_p=\underset{\theta_p}{\arg\max}\sum_{i=1}^n\sum_{k=1}^K\hat{z}_{i,k}\log p_k(\bx_i;\theta_p),
 \end{equation}
 and
 \begin{equation}\label{comp-reg2}
 	\left(\hat{\theta}_\mu^{(k)},\hat{\theta}_\phi^{(k)}\right)=\underset{\theta^{(k)}_\mu,\theta^{(k)}_\phi}{\arg\max}\sum_{i=1}^n\sum_{k=1}^K\hat{z}_{i,k}\log f_k\left(y_i;\mu_k\left(\bx_i;\theta_\mu^{(k)}\right),\phi_k\left(\bx_i;\theta_\phi^{(k)}\right)\right), ~\text{for} ~ k=1,\ldots,K.
 \end{equation}
These optimizations are similar to those in \eqref{p-reg} and \eqref{comp-reg}. 
The only difference is that ${z}_{i,k}\in\{0,1\}$ in \eqref{p-reg} and \eqref{comp-reg} while $\hat{z}_{i,k}\in(0,1)$ in  \eqref{p-reg2} and \eqref{comp-reg2}.
Therefore, in the maximization step we perform a multinomial logistic classification with {\it fractional} response $\hat{z}_{i,k}$ and $K$ {weighted}-regressions fitted to {\it all} samples with weights $\hat{z}_{i,k}$.
%The key of the EM algorithm is to transfer a difficult optimization \eqref{mix-gen} into $K+1$ independent small optimizations \eqref{p-reg2} and \eqref{comp-reg2}.

\section{Expectation-Boosting algorithm}\label{sec:EB}
In the EM algorithm, the functions $p(\bx), \mu_k(\bx), \phi_k(\bx)$ are always assumed to be parametric functions.
To make it possible for non-parametric modeling, we replace the maximization step in the EM algorithm with a boosting step.
The boosting algorithm estimates the regression functions non-parametrically, and thus facilitates more flexible shapes.
With negative log-likelihood function as the loss function in the boosting algorithm, 
the boosting step increases the likelihood at each iteration, but overfitting-sensitively.
The boosting step follows a generic functional gradient descent algorithm. In this section, we first describe the generic functional gradient descent algorithm, then propose the EB algorithm, finally discuss interpretation, hyperparameter tuning and initialization in the EB algorithm.

\subsection{Generic functional gradient descent algorithm}
Suppose a to-be-boosted {non-parametric regression function} as $F:\mathcal{X}\rightarrow\R$. In boosting algorithm, the goal is to  estimate $F$ to minimize the expected loss $$\hat{F}=\underset{F}{\arg\min}\E\left[C(Y,F(\bx))\right],$$
	where $C:\R\times\R\rightarrow\R_+$ is the \textit{loss function}.
	The expected loss is always replaced by sample average loss:
	$$\hat{F}=\underset{F}{\arg\min}\frac{1}{n}\sum_{i=1}^nC(y_i,F(\bx_i)).$$
	In our setting of statistical modeling, the loss function depends on the corresponding likelihood function.
	We choose the \textit{negative log-likelihood} function for the loss function. Hence, minimizing the loss function is equivalent to maximizing the likelihood.
For example, in LogitBoost the loss function is negative binomial log-likelihood
$$C(Y,F)=\log(1+\exp(-2YF)), ~~ Y\in\{-1,1\},$$
where 
\begin{equation}\label{logit-link}
F(\bx)=\frac{1}{2}\log\left(\frac{\Pr[Y=1|\bx]}{\Pr[Y=-1|\bx]}\right).
\end{equation}
In $L_2$Boost, the loss function is negative Gaussian log-likelihood
\begin{equation}\label{l2}
	C(Y,F)=(Y-F)^2/2, ~~ Y\in \R,
\end{equation}
where $F(\bx)=\E(Y|\bx).$

In the GLM framework, the link function $g:\mathcal{Y}\rightarrow\R$ is a mapping from the parameters of interest (e.g. mean or probability odds) to  the linear predictor.
Here, we apply the same terminology where the link function is a mapping from the parameters of interest to the to-be-boosted non-parametric regression function $F$.
In LogitBoost, equation \eqref{logit-link} implies a half logged probability odds link function $g:\R_+\rightarrow\R$ 
$$g(\text{odds})=\frac{1}{2}\log(\text{odds})=\frac{1}{2}\log\left(\frac{\Pr[Y=1|\bx]}{\Pr[Y=-1|\bx]}\right)=F(\bx).$$
In $L_2$Boost, we have an identify link function $g:\R\rightarrow\R$
$$g(\text{mean})=\text{mean}=\E(Y|\bx)=F(\bx).$$
In our setting of statistical modeling, we first specify the likelihood function and the link function, then derive the loss function.

%	Commonly used {link functions} and loss functions in different boosting algorithms are listed below:
%	\begin{itemize}
%		\item AdaBoost: $$C(y,F)=\exp(yF),  ~~ y\in\{-1,1\},$$
%		$$F(\bx)=\frac{1}{2}\log\left(\frac{\Pr[Y=1|\bx]}{\Pr[Y=-1|\bx]}\right);$$

%		\item $L_2$Boost: 
%		\begin{equation}\label{l2}
%			C(y,F)=(y-F)^2/2, ~~ y\in \R,
%		\end{equation}
%		$$F(\bx)=\E(Y|\bx).$$
%	\end{itemize}

We follow \citet{buehlmann:2003} to briefly describe the generic functional gradient descent algorithm in Algorithm \ref{alg1}, which will serve as building blocks in the proposed EB algorithm. 
The algorithm estimates $F$ in an additive form
$$\hat{F}(\bx)=\hat{F}^{[0]}(\bx)+\sum_{m=1}^M\hat{f}^{[m]}(\bx),$$
where $\hat{F}^{[0]}$ is the initial value and $\hat{f}^{[m]}$ is called the $m$-th weak learner which involves only a small number of covariates.

\begin{algorithm}[htp!] % enter the algorithm environment
	\caption{The generic functional gradient decent algorithm.} % give the algorithm a caption
	\label{alg1} % and a label for \ref{} commands later in the document
	\begin{algorithmic}[1] % enter the algorithmic environment
		\STATE Initialization. Set a suitable initial value $\hat{F}^{[0]}(\bx).$  
		\FOR{$m=1$ to $M$}
		\STATE Projection of gradient to weak learner.
		Calculate \textit{negative gradient} $$u_i=\left.-\frac{\partial C(y_i,F)}{\partial F}\middle|_{F=\hat{F}^{[m-1]}(\bx_i)}\right., i=1,\ldots,n.$$
		Data $(u_i,\bx_i)_{i=1:n}$ is used to calibrate a weak learner $\hat{f}^{[m]}(\bx;\hat{\theta}^{[m]})$ with \textit{loss function $L_2$} \eqref{l2}.
		\STATE One-dimensional optimization and update.
		Solve a one-dimensional optimization  to find \textit{expansion coefficient} $\hat{\omega}^{[m]}$:
		$$\hat{\omega}^{[m]}=\underset{\omega}{\arg\min}\sum_{i=1}^n C(y_i, \hat{F}^{[m-1]}(\bx_i)+\omega\hat{f}^{[m]}(\bx_i)).$$
		Update $$\hat{F}^{[m]}=\hat{F}^{[m-1]}+s\hat{\omega}^{[m]}\hat{f}^{[m]},$$
		where $s$ is \textit{shrinkage factor} (learning rate).
		\ENDFOR
		\RETURN $\hat{F}(\bx)=\hat{F}^{[M]}$.
	\end{algorithmic}
\end{algorithm}
%	\begin{enumerate}
%		\item Initialization: Set a suitable initial value $\hat{F}^{[0]}(\bx).$ 
%		Let $m=0.$
%		\item Projection of gradient to weak learner:
%		Calculate \textit{negative gradient} $$u_i=\left.-\frac{\partial C(y_i,F)}{\partial F}\middle|_{F=\hat{F}^{[m]}(\bx_i)}\right., i=1,\ldots,n.$$
%		Data $(u_i,\bx_i)_{i=1:n}$ is used to calibrate a weak learner $\hat{f}^{[m+1]}(\bx;\hat{\theta}^{[m+1]})$ with \textit{loss function $L_2$} \eqref{l2}.{\color{blue} [The weaker learner is not defined or described, and appear in the algorithm at the first time. It is better to put this sentence in the article and give this step a straightforward description.]}
%	
%		\item One-dimensional optimization:
%		Solve a one-dimensional optimization  to find \textit{expansion coefficient} $\hat{\omega}^{[m+1]}$:
%		$$\hat{\omega}^{[m+1]}=\underset{\omega}{\arg\min}\sum_{i=1}^n C(y_i, \hat{F}^{[m]}(\bx_i)+\omega\hat{f}^{[m+1]}(\bx_i))$$
%		Update $$\hat{F}^{[m+1]}=\hat{F}^{[m]}+s\hat{\omega}^{[m+1]}\hat{f}^{[m+1]},$$
%		where $s$ is \textit{shrinkage factor} (learning rate).
%		\item Iteration: Let $m$ increase by 1, and repeat steps 2-3.
%			\end{enumerate}

In the algorithm, weak learners are fitted to a negative gradient $U$ rather than $Y$.
		 The loss function in weak learners is always $L_2$, independently with model loss function $C$. 
		 If decision trees \citep{breiman1983classification} are used as weak learners, the algorithm is called \textit{gradient boosting decision tree} (GBDT), where {calibration of decision trees and variable selection} are performed simultaneously.
		 One may refer to \citet{hastie2009elements} for decision trees and the recursive partitioning algorithm. Note that
		\citet{buhlmann2007boosting} argues that step 4 of one-dimensional optimization seems unnecessary given the learning rate $s$ is sufficiently small according to some empirical experiments.




\subsection{Expectation-Boosting algorithm}

%We denote the parameters of $p,\mu,\phi$ as functions {$F,G,H$}, respectively.
In the EB algorithm, we follow the same expectation step as in the EM algorithm and replace the maximization step by boosting.
The boosting algorithm is applied to estimate mixing probabilities and component parameters given the conditional expectation of latent component indicator variables $\hat{z}_{i,k}$.
Before applying a boosting algorithm, we need to specify the likelihood function and the link function, which determine the loss function.
In mixture models, the log-likelihood function is already given by \eqref{likelihood}.
Therefore, we only need to specify the link functions for mixing probabilities, component means and component dispersions, respectively.
%In the EB algorithm, we need to specify link functions and loss functions, which determines negative gradient function.

For mixing probabilities $p_1(\bx),\ldots,p_K(\bx)$, they are mapped to the to-be-boosted regression functions $F_1(\bx),\ldots,F_K(\bx)$ by the multiple logistic link function $g_k:\mathcal{P}\rightarrow \R$ such that 
\begin{equation}\label{inv-logistic}
		g_k(p_1(\bx),\ldots,p_K(\bx))=\log p_k(\bx)-\frac{1}{K}\sum_{l=1}^K\log p_l(\bx)=F_k(\bx),~~k=1,\ldots,K.
\end{equation}
or equivalently
	\begin{equation}\label{logistic}
	g_k^{-1}(F_1(\bx),\ldots,F_K(\bx))=\frac{\exp\left(F_k(\bx)\right)}{\sum_{l=1}^{K}\exp\left(F_l(\bx)\right)}=	p_k(\bx),~~k=1,\ldots,K.
	\end{equation}
Given the negative log-likelihood function \eqref{p-reg2} of multinomial distribution, we derive the loss function as
\begin{equation}\label{p-loss}
		{C_{0}\left((Z_k, F_k(\bx))_{k=1:K}\right)}= - \sum_{k=1}^K Z_k \log p_k(\bx).
\end{equation}
The negative gradient of the loss function $C_0$ w.r.t. $F_k$ is given by
\begin{equation}\label{p-gradient}
	{U_k\left((Z_k, F_k(\bx))_{k=1:K}\right)}=-\frac{\partial C_0\left((Z_k, F_k(\bx))_{k=1:K}\right)}{\partial F_k(\bx)}=
	Z_k-p_k(\bx), ~~k=1,\ldots,K.
\end{equation}

For the component parameters $\mu$ and $\phi$,  we denote the to-be-boosted regression functions by $G(\bx)$ and $H(\bx)$, respectively. 
The link functions depend on the component models.  For example, in a component Gaussian model we assume $\mu_k(\bx)=G_k(\bx), \log \phi_k(\bx)=H_k(\bx)$; 
in a component Poisson model $\log\mu_k(\bx)=G_k(\bx)$; 
in a component gamma model $\log \mu_k(\bx)=G_k(\bx), \log \phi_k(\bx)=H_k(\bx).$
%Note that we slightly abuse function notations $p_k,\mu_k,\phi_k$ (i.e. the inverse link function); in Section \eqref{review:mix1} they define different functions.
With the negative log-likelihood function \eqref{comp-reg2}, we derive the loss function as
	\begin{equation}\label{comp-loss}
		{C_k(Y,Z_k,G_k(\bx))}= -Z_k\log f_k(Y;\mu_k(\bx),\phi_k), ~ k=1,\ldots,K,
	\end{equation}
where $Z_k$ is treated as a weight multiplying to the usual negative log-likelihood function $-\log f_k$.
	The negative gradient of loss function $C_k$ w.r.t. $G_k$ is given by 
\begin{equation}\label{comp-gradient}
{V_k(Y,Z_k,G_k(\bx))}=-\frac{\partial C_k(Y,Z_k,G_k(\bx))}{\partial G_k(\bx)}.
\end{equation}
Note that we have assumed that the dispersion $\phi_k$ is fixed among samples (not related to $\bx$). 
	%\blue{The extension to dispersion modeling is straightforward; please refer to the real data analysis in Section \ref{sec:third example}.}
We will discuss dispersion modeling later.
Upon the above defined notations, we are ready to introduce the proposed EB algorithm.
We first summarize the proposed EB algorithm in Algorithm \ref{EB}.

\begin{algorithm}[htp!]
	\caption{The Expectation-Boosting algorithm.} % give the algorithm a caption
	\label{EB} % and a label for \ref{} commands later in the document
	\begin{algorithmic}[1]
	\STATE Initialization. Set  $\hat{p}^{[0]}_k,\hat{\mu}^{[0]}_k,\hat{\phi}_k^{[0]}$, $k=1,\ldots,K$. 
	\FOR{$t=1$ to $T$}
	\STATE Expectation. Calculate $\hat{z}^{[t]}_{i,k}$ given $\hat{p}^{[t-1]}_k,\hat{\mu}^{[t-1]}_k,\hat{\phi}^{[t-1]}_k$, $k=1,\ldots,K$. 
	%given $\hat{p}_k^{[t-1]}(\bx),\hat{\mu}_k^{[t-1]}(\bx),\hat{\phi}_k^{[t-1]}$, $k=1,\ldots,K$.
	\STATE Initialization of mixing probabilities. Set  $\hat{p}_k^{[t,0]}, \hat{F}_k^{[t,0]}$, $k=1,\ldots, K$.
		\FOR{$m=1$ to $M_0$}
		\FOR{$k=1$ to $K$}
		\STATE Projection of gradient to weak learner. Calibrate a weak learner $\hat{f}_k^{[t,m]}$. 
		\STATE One dimensional optimization and update. Update $\hat{F}_k^{[t,m]}$ using $\hat{F}_k^{[t,m-1]},\hat{f}_k^{[t,m]}$.
		\ENDFOR
		\RETURN{$\hat{p}_k^{[t,m]}$ according to \eqref{logistic}, $k=1.\ldots,K$.}
	   \ENDFOR
     	\RETURN{$\hat{F}_k^{[t]}=\hat{F}_k^{[t,M_0]}, \hat{p}_k^{[t]}=\hat{p}_k^{[t,M_0]}$, $k=1,\ldots,K.$}
	\STATE Initialization of component means. Set  $\hat{\mu}_k^{[t,0]}, \hat{G}_k^{[t,0]}$, $k=1,\ldots,K$. 
	\FOR{$k=1$ to $K$}
		\FOR{$m=1$ to $M_k$}
			\STATE Projection of gradient to weak learner. Calibrate a weak learner $\hat{g}_k^{[t,m]}$.
		\STATE One dimensional optimization and update.	Update $\hat{G}_k^{[t,m]},\hat{\mu}_k^{[t,m]}$ using $\hat{G}_k^{[t,m-1]},\hat{g}_k^{[t,m]}$.
		\ENDFOR
		%\RETURN{$\hat{\mu}_k^{[t,m]}$ according to corresponding link functions.} 
		     	\RETURN{$\hat{G}_k^{[t]}=\hat{G}_k^{[t,M_k]}, \hat{\mu}_k^{[t]}=\hat{\mu}_k^{[t,M_k]}.$}
	\ENDFOR
	\RETURN{$\hat{G}_k^{[t]}, \hat{\mu}_k^{[t]}, k=1,\ldots,K.$}

	\STATE Estimation of dispersion. Calculate the MLE $\hat{\phi}^{[t]}_k$ given $\hat{\mu}^{[t]}_k$ and $\hat{z}^{[t]}_k$, $k=1,\ldots,K$.
	\ENDFOR
	\RETURN{$\hat{p}^{[T]}_k,\hat{\mu}^{[T]}_k,\hat{\phi}^{[T]}_k$, $k=1,\ldots,K$.}
	\end{algorithmic}
\end{algorithm}
%\begin{enumerate}
%	\item[1] Initialize $\hat{p}^{[0]},\hat{\mu}^{[0]},\hat{\phi}^{[0]}$. Set $t=0$.
%	\item[2] Calculate conditional expectation of latent variable $\hat{z}^{[t+1]}$ given $\hat{p}^{[t]},\hat{\mu}^{[t]},\hat{\phi}^{[t]}$.
%	\item[3.1]  Gradient boosting mixing probabilities $\hat{p}^{[t+1]}$  given latent variable $\hat{z}^{[t+1]}$.
%	\item[3.2] Gradient boosting component parameters $\hat{\mu}^{[t+1]}$ given latent variable $\hat{z}^{[t+1]}$.
%	\item[4]  Calculate the MLE $\hat{\phi}^{[t+1]}$ given $\hat{\mu}^{[t+1]}$ and $\hat{z}^{[t+1]}$. Increase $t$ by 1. Repeat steps 2-3 until $t$ reaches to $T$.
%\end{enumerate}

In Algorithm \ref{EB}, we have a hierarchical loop: the outer EB iteration lines 2-23 and the inner boosting iteration lines 5-11 and 14-20, respectively.
The loop of inner boosting iteration follows a similar course of Algorithm \ref{alg1}, which is nested inside the loop of outer EB iteration.
Apparently, the EB algorithm takes more computation time than the EM algorithm. However, the EB algorithm can be accelerated by parallel computing.
In lines 6-9, the functions $(F_k)_{k=1:K}$ are boosted independently. 
In lines 14-20, the component means are boosted independently.
Thus, those independent steps can be conducted simultaneously to reduce running time; see Table \ref{gaussian-summary}.
Notice that the two loops in lines 14-20 and 15-18 are exchangeable, since $\hat{\mu}_k^{[t,m]}$ only depends on $\hat{G}_k^{[t,m]}$. However, the two loops in lines 5-11 and 6-9 are not exchangeable since $\hat{p}_k^{[t,m]}$ depends on all $(\hat{F}_k^{[t,m]})_{k=1:K}$ via equation \eqref{logistic}. In the EB algorithm, we have assumed that dispersion $\phi_k$ is fixed among samples (not related to $\bx$). The extension to joint modeling of mean and dispersion is not straightforward since they are not independent; please refer to \citet{jorgensen:1997} for dispersion modeling.
Joint modeling of both mean and dispersion is not considered in this paper. 
% by adding another gradient boosting step 3.3 for component model parameter $\phi$ and removing the MLE calculation in step 4.

In the following, we present more details for each step of the EB algorithm in Algorithm \ref{EB}.

{\bf 1. Initialization of EB algorithm (line 1)}. Set $\hat{p}_k^{[0]},\hat{F}_k^{[0]}, k=1,\ldots,K$ as:
$$\hat{p}_k^{[0]}=\frac{1}{K},~~
\hat{F}_k^{[0]}=\log \hat{p}_k^{[0]}-\frac{1}{K}\sum_{l=1}^K\log \hat{p}_l^{[0]}, ~~ k=1,\ldots,K.$$
	Set $\hat{\mu}_k^{[0]},\hat{G}_k^{[0]}, k=1,\ldots,K,$ as:
	$$\hat{\mu}_k^{[0]}=\frac{\sum_{i=1}^nY_i}{n},~~\hat{G}_k^{[0]}=\mu_k^{-1}(\hat{\mu}_k^{[0]}),~~ k=1,\ldots,K,$$
	where $\mu^{-1}_k$ denotes the link function in the component model $k$.
		Set $\hat{\phi}_k^{[0]},k=1,\ldots,K$ as the sample variance. 
%Note that step 1 will not be repeated in the algorithm.	


{\bf 2. Expectation (line 3)}. We can calculate the conditional expectation of latent component indicator variable given  $\hat{p}^{[t-1]}_k,\hat{\mu}^{[t-1]}_k,\hat{\phi}^{[t-1]}_k$, $k=1,\ldots,K$:
	\begin{equation*}
		\hat{z}_{i,k}^{[t]}=\frac{\hat{p}_{k}^{[t-1]}(\bx_i) f_{k}\left(y_i ; \hat{\mu}_{k}^{[t-1]}(\bx_i), \hat{\phi}_k^{[t-1]} \right)}{\sum_{l=1}^{K} \hat{p}_{l}^{[t-1]}(\bx_i) f_{l}\left(y_i ; \hat{\mu}_{l}^{[t-1]}(\bx_i), \hat{\phi}_l^{[t-1]}\right)},~~ i=1,\ldots,n, k=1,\ldots,K.
	\end{equation*}
Note that the conditional expectation $\hat{z}_{i,k}^{[t]}$ carries the information obtained from the last EB iteration $t-1$,
which then  interacts with the boosting via the negative gradients \eqref{p-gradient} and \eqref{comp-gradient} and the loss functions \eqref{p-loss} and \eqref{comp-loss} at this EB iteration $t$.

{\bf 3. Initialization of mixing probabilities (line 4)}. 
Set $\hat{p}_k^{[t,0]},\hat{F}_{k}^{[t,0]}$, $k=1,\ldots,K$ as
		\begin{equation}\label{ini-1}			
			\begin{aligned}
			\hat{p}_k^{[t,0]}=&\frac{1}{K}, ~k=1,\ldots,K,\\
		\hat{F}_k^{[t,0]}=&\log \hat{p}_k^{[t,0]}-\frac{1}{K}\sum_{l=1}^K\log \hat{p}_l^{[t,0]}, ~ k=1,\ldots,K.
		\end{aligned}
	\end{equation}
		In contrast to the initialization in line 1, the current one will be repeated at each outer EB iteration $t$. 
		Later we will discuss another initialization strategy which considers the boosted values at the previous EB iteration.
		%In the inner loop of boosting step 3.1, this initialization will not be repeated; see step 3.1.4. 
		
{\bf 4. Projection of gradient to weak learner (line 7)}.
		We compute the negative gradient for each data point $i=1,\ldots,n$ as
		$$u_{i,k}^{[t,m]}=U_k\left(\left(\hat{\bz}_{i,k}^{[t]},\hat{F}_k^{[t,m-1]}(\bx_i)\right)_{k=1:K}\right)=\hat{z}_{i,k}^{[t]}-\hat{p}_{i,k}^{[t,m-1]}.$$
		The data $(u_{i,k}^{[t,m]},\bx_i)_{i=1:n}$ is used to calibrate a $L$-terminal node regression tree with $L_2$ loss $$\hat{f}_k^{[t,m]}\left(\bx;R^{[t,m]}_{k,l=1:L},\bar{u}^{[t,m]}_{k,l=1:L}\right),$$
		where $R^{[t,m]}_{k,l=1:L}$ is a partition of covariate space $\mathcal{X}\subset\R^J$ and $\bar{u}^{[t,m]}_{k,l=1:L}$ contains the average gradient in each terminal node.
		Note that for different $k$, the regression trees can have different numbers of terminal nodes. Here we assume $L$-terminal node to simplify the notation.
		
{\bf 5. One dimensional optimization and update (line 8)}.
 It turns out that  the one-dimensional optimization for expansion coefficient leads to the following update:
		$$\hat{F}_k^{[t,m]}(\bx)=\hat{F}_k^{[t,m-1]}(\bx)+s_p\sum_{l=1}^{L}\gamma^{[t,m]}_{k,l}\mathbbm{1}_{R^{[t,m]}_{k,l}}(\bx),$$
		where $s_p$ is the learning rate and
		$$\gamma^{[t,m]}_{k,l}=\frac{K-1}{K}\frac{\sum_{\{i:\bx_i\in R_{k,l}^{[t,m]}\}}u_{i,k}^{[t,m]}}{\sum_{\{i:\bx_i\in R_{k,l}^{[t,m]}\}}\left|u_{i,k}^{[t,m]}\right|\left(1-\left|u_{i,k}^{[t,m]}\right|\right)}, ~l=1,\ldots,L.$$
Please refer to equation (32) in \citet{friedman2001greedy}. Note that for different $k$, the learning rate can be different. 

{\bf 6. Return $\hat{p}_k^{[t,m]}$ according to \eqref{logistic}, $k=1,\ldots,K$ (line 10)}.
The mixing probabilities are updated as
$$\hat{p}_k^{[t,m]}(\bx)=\frac{\exp\left(\hat{F}^{[t,m]}_k(\bx)\right)}{\sum_{l=1}^{K}\exp\left(\hat{F}^{[t,m]}_l(\bx)\right)},~~ k=1,\ldots,K.$$

Lines 4-12 are a typical $K$-class logistic gradient boosting. 
In lines 6-9, $K$ independent boostings are performed for $(F_k)_{k=1:K}$. Hence, we can accelerate lines 6-9 by parallel computing.
To the best of our knowledge, there is no available R package supporting a multinomial logistic boosting with fractional response $\hat{\bz}_{i}\in\mathcal{P}\subset(0,1)^K$.
So we code lines 4-12 from bottom. Only the R package {\tt rpart} is called to apply the recursive partition algorithm to calibrate the weak learner $\hat{f}_k^{[t,m]}$ in line 7.

{\bf 7. Initialization of component means (line 13)}. Set $\hat{\mu}_k^{[t,0]},\hat{G}_k^{[t,0]}$, $k=1,\ldots,K$ as:
			\begin{equation}\label{ini-2}
				\hat{\mu}_k^{[t,0]}=\frac{\sum_{i=1}^ny_i}{n},~ \hat{G}_k^{[t,0]}=\mu_k^{-1}(\hat{\mu}_k^{[t,0]}), k=1,\ldots,K,
					\end{equation}
		where $\mu^{-1}_k$ denotes the link function in the component model $k$. 
		Similar to line 4, this initialization will be repeated at each outer EB iteration $t$. Later we will discuss another initialization strategy which considers the boosted values at the previous EB iteration.
	
{\bf 8. Projection of gradient to learner (line 16)}.
		We compute the negative gradient for each data point $i=1,\ldots,n$ as 
		$$v_{i,k}^{[t,m]}=V_k(y_i,\hat{z}^{[t]}_{i,k},\hat{G}_k^{[t,m-1]}(\bx_i)).$$
		The data $(v_{i,k}^{[t,m]},\bx_i)_{i=1:n}$ is used to calibrate a $L$-terminal node regression trees with $L_2$ loss $$\hat{g}_k^{[t,m]}\left(\bx;S^{[t,m]}_{k,l=1:L},\bar{v}^{[t,m]}_{k,l=1:L}\right),$$
		 where $S^{[t,m]}_{k,l=1:L}$ is the partition of covariate space $\mathcal{X}\subset\R^J$ and $\bar{v}^{[t,m]}_{k,l=1:L}$ contains the average gradient in each terminal node.
		 Note that the number of terminal nodes $L$ can be different for different component models. 
		
{\bf 9. One dimensional optimization and update (line 17)}.	
	We conduct the following one-dimensional optimizations to find the best expansion coefficients:
		$$\hat{w}_{k}^{[t,m]}=\underset{w}{\arg\min}\sum_{i=1}^n C_{k}(y_i,\hat{z}_{i,k}^{[t]},\hat{G}_k^{[t,m-1]}(\bx_i)+w\hat{g}_k^{[t,m]}(\bx_i)).$$
		And then we update
		$$\hat{G}_k^{[t,m]}(\bx)=\hat{G}_k^{[t,m-1]}(\bx)+s_\mu\hat{w}_{k}^{[t,m]}\hat{g}_{k}^{[t,m]}(\bx),$$
		where $s_\mu$ is the learning rate. Note that for different component models the learning rates can be different. 
	Update the component mean as
$$\hat{\mu}_k^{[t,m]}(\bx)=\mu_k(\hat{G}_k^{[t,m]}(\bx)),$$
where $\mu_k$ denotes the inverse link function in component model $k$.
%{\bf 10. Return {$\hat{G}_k^{[t]}=\hat{G}_k^{[t,M_k]}, \hat{\mu}_k^{[t]}=\hat{\mu}_k^{[t,M_k]}$} (line 19)}. The component means are updated as
	
Lines 13-21 implement $K$ independent weighted gradient boostings with weights $\hat{z}_{i,k}^{[t]}$ for each component mean.
We can accelerate this loop  by conducting the $K$ boostings simultaneously.
We rely on the R packages {\tt gbm} or {\tt mboost} to perform the boosting in this loop. 
Those two packages can facilitate boosting with additional weights.


{\bf 10. Estimation of dispersion (line 22)}. 
The MLE $\hat{\phi}_k^{[t]}$ is derived given the conditional expectation $\hat{z}_{i,k}^{[t]}$ and the component mean $\hat{\mu}_k^{[t]}$:
\begin{equation}
		\hat{\phi}_k^{[t]}=\underset{\phi_k}{\arg\max} \sum_{i=1}^n\hat{z}_{i,k}^{[t]}\log f_k(y_i;\hat{\mu}_k^{[t]}(\bx_i),\phi_k), ~ k=1,\ldots,K.
\end{equation}

In summary, a total of $(KM_0+M_1+\ldots,+M_K)\times T$ weak learners are calibrated in the EB algorithm. 
In lines 6-9, $(F_k)_{k=1:K}$ can be boosted simultaneously at each $m$.
Furthermore, in lines 14-20, the $K$ boosting algorithms can be conducted simultaneously at each $t$. 
Such parallel computing will accelerate the EB algorithm significantly; see Table \ref{gaussian-summary}.

Gradient boosting with regression trees inherits the desirable features of trees.
One of the advantages is automated feature engineering with regard to non-linearity, variable transformation, and interaction. 
First, a tree produces a piecewise constant function facilitating non-linear effects of a covariate.
Second, trees are invariant under monotone transformations of the individual covariate. 
For example, using $x_j,\log x_j, \exp x_j$ as the $j$-th covariate leads to the same result.
So there is no need to consider individual variable transformation.
Third, a tree with $L$ terminal nodes produces a function with interaction order at most $L-1$.
Another advantage of trees is internal variable selection. Trees tend to be robust against irrelevant covariates.
Moreover, trees are overfitting-sensitive given a suitable complexity parameter, i.e., any split not sufficiently improving the fit will not be attempted. 

The boosting also mitigates many undesirable features of the single tree model. 
Small trees are inaccurate due to the coarse nature of the piecewise constant functions. In contrast, large trees are not stable due to the high-order interaction involved.
All of these are mitigated by boosting, which produces much finer piecewise constant functions; it enhances stability by using small trees and averaging over many of them.
One remarkable feature of boosting is the resistance to overfitting, which owes to overfitting-sensitive split in trees and two regularization hyperparameters of the learning rate $s$ and the number of iterations $M$.
We usually specify a large $M$ and early stop iterating by minimizing the validation loss before reaching $M$.
For a sufficiently small learning rate, we usually do not observe a typical tick shape of validation loss but a leveling-off pattern; see Figure 1 in \cite{friedman2001greedy}.

\subsection{Interpretation, hyperparameter tuning and initialization in the EB algorithm}
A particularly useful interpretative tool for boosting is relative importance proposed by \cite{breiman1983classification}.
In mixture models, we consider the relative importance of covariates in each component and the overall relative importance.
Considering the weak learner of tree $\hat{f}_k^{[T,m]}$ at the last EB iteration $T$,  the relative importance of covariate $x_j$ is
\begin{equation}\label{influence}
	I_j(\hat{f}_k^{[T,m]})=\sum_{l=1}^{L-1}{e}_l\mathbbm{1}_{d_l}(j), ~~ j=1,\ldots,J,
\end{equation}
where the summation is over the non-terminal nodes $l$ of the $L$-terminal node tree $\hat{f}^{[T,m]}_k$, $d_l$ is the splitting variable associated with node $l$, and $e_l$ is the corresponding empirical improvement in squared error as a result of the split given by 
$$e_l=\frac{w_\text{left}w_\text{right}}{w_\text{left}+w_\text{right}}\times(m_\text{left}-m_\text{right})^2.$$
Here, $m_\text{left}$ and $m_\text{right}$ are the gradient means in the left and right splits, respectively, 
and $w_\text{left}$ and $w_\text{right}$ are the corresponding sums of the weights.
Considering a collection of trees $(\hat{f}^{[T,m]}_k)_{m=1:M_0}$ in the $k$-th mixing probability $\hat{p}_k^{[T]}$, \eqref{influence} can be generalized by its average over all of the trees
\begin{equation}
	IP_{j,k}=\frac{1}{M_0}\sum_{m=1}^{M_0} I_j(\hat{f}_k^{[T,m]}), ~~ j=1,\ldots,J,
\end{equation}
which is interpreted as the relative importance of $x_j$ in separating component $k$ from the other components.
The overall relative importance of $x_j$ is obtained by averaging over all components:
$$
IP_{j}=\frac{1}{K}\sum_{k=1}^K IP_{j,k}, ~~ j=1,\ldots,J.
$$

The above formulae are about the relative importance of covariates in mixing probabilities. Similarly, we can obtain the relative importance of covariates in each component mean as
\begin{equation}
	IM_{j,k}=\frac{1}{M_k}\sum_{m=1}^{M_k} I_j(\hat{g}_k^{[T,m]}),~~ j=1,\ldots,J,
\end{equation}
and the overall relative importance of covariates in means as
$$
IM_{j}=\frac{1}{K}\sum_{k=1}^K IM_{j,k}, ~~j=1\ldots,J.
$$

The EB algorithm has four types of hyperparameters: the number of EB iterations $T$, the numbers of boosting iterations $M_0$ and $(M_k)_{k=1:K}$, the learning rates $s_p$ and $s_\mu$, and hyperparameters in weak learners. 
Although there are methods to systematically tune hyperparameters, such as grid research and Bayesian optimization, tuning hyperparameters in many practical algorithms is more art than a science.
In predictive modelling with hyperparameters, we usually split the data into three mutually exclusive sets, training set, validation set and test set, whose indices are denoted by $\mathcal{I}_\text{train}$, $\mathcal{I}_\text{val}$ and  $\mathcal{I}_\text{test}$, respectively.

The number of EB iterations $T$ can be tuned by screening the trace plot of the log-likelihood on the learning data $\mathcal{I}_\text{learn}=\mathcal{I}_\text{train}\cup\mathcal{I}_\text{val}$ at each iteration $t$:
$$\sum_{i\in\mathcal{I}_\text{learn}}\log\left(\sum_{k=1}^K\hat{p}_k^{[t]}(\bx_i)f_k\left(y_i;\hat{\mu}_k^{[t]}(\bx_i),\phi_k^{[t]}\right)\right).$$
If there is no noticeable improvement, the EB algorithm converges.

For the numbers of boosting iterations  {$M_0,(M_k)_{k=1:K}$}, we specify a sufficiently large maximal iteration number and calculate the following $K+1$ validation losses on $\mathcal{I}_\text{val}$ at each iteration $m$:
\begin{equation}\label{loss-p}
-\sum_{i\in\mathcal{I}_\text{val}}\sum_{k=1}^K \hat{z}_{i,k}^{[t]}\log \hat{p}_k^{[t,m]}(\bx_i),
\end{equation}
\begin{equation}\label{loss-mu}
-\sum_{i\in\mathcal{I}_\text{val}} \hat{z}^{[t]}_{i,k}\log f_k\left(y_i;\hat{\mu}_k^{[t,m]}\left(\bx_i\right),\hat{\phi}_k^{[t]}\right), ~ k=1,\ldots,K.
\end{equation}
If there is no noticeable improvement of the validation loss, the corresponding boosting can be early stopped before reaching the maximal iteration number.  

For the learning rates {$s_p$} and $s_\mu$, lower learning rates tend to lead to a better fitting and predictive performance but require more boosting iterations.
We adjust the learning rates by monitoring the validation loss \eqref{loss-p} and \eqref{loss-mu}, respectively.
Note that we can specify different learning rates $s_\mu$ for different component means.

The hyperparameters in weak learners of regression trees include complexity parameters, maximum depth, number of terminal nodes, etc.
Those hyperparameters can be tuned as in typical regression trees. One may refer to \citet{hastie2009elements} for more details.
%In the following applications, we particularly focus on two hyperparameters, learning rates and maximum depth. 

In lines 4 and 13 of the initialization of boosting, 
we initialize parameters at iteration $t$ {independently} with the previously boosted estimates $\hat{p}_k^{[t-1]}, \hat{\mu}_k^{[t-1]}$; see equations \eqref{ini-1} and \eqref{ini-2}. We call this {\it uncorrelated boosting}. However, one may spot that boosting at iteration $t$ is not independent with boosting at iteration $t-1$ 
since boosting at iteration $t$ is based on $\hat{\bz}_i^{[t]}$ which depends on the boosted results at iteration $t-1$. The gains from the boosting at iteration $t-1$ are passed to  the boosting at iteration $t$ via the conditional expectation $\hat{\bz}_i^{[t]}$. 
	
We might initialize parameters at iteration $t$ as the previously boosted estimates $$\hat{p}_k^{[t,0]}=\hat{p}_k^{[t-1]}, \hat{\mu}_k^{[t,0]}=\hat{\mu}_k^{[t-1]}.$$
It would lead to fewer inner boosting iterations $M_0,(M_k)_{k=1:K}$ (i.e., an {earlier} stop on the validation loss) since the boosting starts from better initial values.
	We call this {\it forward boosting}. 
	However, with forward boosting, it is impossible to predict for new data using the fitted model at the last iteration $(\hat{p}_k^{[T]},\hat{\mu}_k^{[T]},\hat{\phi}_k^{[T]})_{k=1:K}$ due to the {iterative initialization}. 
	One solution is to apply an additional boosting step with default initialization \eqref{ini-1} and \eqref{ini-2} given the {last conditional expectations of the latent variables $\hat{\bz}^{[T]}_i$}. 
	In the following applications, we only implement uncorrelated boosting.

	
\section{Applications}\label{sec:application}

In this section, we conduct two simulation studies and one real data analysis. 
The first simulated data follows a zero-inflated Poisson model, which mimics claim count data with an excess of zeros.
We use this example to illustrate the advantages of the EB algorithm over the EM algorithm.
The second simulated data follows a mixture of Gaussian models. 
We compare different mixing structures and  investigate parallel computing in this example.
The real data contains the claims amount data of an insurance company. 
This example demonstrates how to choose component models and mixing structures in practice.

\subsection{First simulated example: zero-inflated Poisson model}

We consider a simulated data set generated from a zero-inflated Poisson (ZIP) model. This simulated data mimics the numbers of claims with an excess of zeros.
The underlying  model is given by:
\begin{equation*}
	f_{\text{ZIP}}(N;\lambda,\pi_0) = \left\{ 
	\begin{array}{ccl}
		\pi_0+(1-\pi_0)e^{-\lambda} & \mbox{for}
		& N=0 \\
		(1-\pi_0)\frac{e^{-\lambda}\lambda^N}{N!} & \mbox{for} &N\in\N_+.
	\end{array}\right.
\end{equation*}
The ZIP model is a mixture of a probability mass of 1 at 0 and a Poisson distribution. The mixing probabilities are $\pi_0$ and $1-\pi_0$.
The probability density function can be written as
$$f_\text{ZIP}(N;\lambda,\pi_0)= \pi_0\mathbbm{1}_{0}(N) + 
(1-\pi_0)\frac{e^{-\lambda}\lambda^N}{N!}.$$
Suppose that five covariates $\bx=(x_1,\ldots,x_5)$ have systematic effects on both $\pi_0$ and $\lambda$:
\begin{equation*}
	\pi_0(\bx)=\frac{\exp \left(F(\bx)\right)}{1+\exp\left( F(\bx)\right)}, ~~
	\lambda(\bx)=\exp\left( G(\bx)\right), 
\end{equation*}
where
\begin{equation*}
	F(\bx)=0.3-2x_2^2+x_2+0.2x_5, ~~G(\bx)=\log 0.5+x_1^2 + 0.2\log x_3 - 0.2x_1 x_4.
\end{equation*}
The covariates are generated from the following distributions:
\begin{align*}
	x_1~&\sim~N(0,0.5^2),\\
	x_2~&\sim~U(0,1), \\
	x_3~&\sim~\Gamma(2,0.5), \\
	x_4~&\sim~Bernulli(0.5),\\
	x_5~&\sim~Bernulli(0.2).
\end{align*}
Note that we use shape-rate parameters for gamma distribution $\Gamma$.
We generate for $n=10,000$ samples $(N_i,\bx_i)_{i=1:n}$, which are partitioned into a learning set $i\in\mathcal{I}_\text{learn}$ of sample size $8,000$ and a test set $i\in\mathcal{I}_{test}$ of sample size $2,000$. 
%The empirical proportion of zero claims in the test data is $83.10\%$. 
If a Poisson distribution is fitted to the learning data, the MLE of Poisson mean is $1/|\mathcal{I}_\text{learn}|\sum_{i\in\mathcal{I}_\text{learn}}N_i=0.2136$, implying the estimated proportion of zero claims as $\exp(-0.2136)=80.77\%$ less than the empirical proportion $83.10\%$ in the test data. 

We will compare different models in terms of test loss defined as the average negative log-likelihood on the test data:
\begin{equation}\label{zip-test-loss}
	-\frac{1}{|\mathcal{I}_\text{test}|}\sum_{i\in\mathcal{I}_\text{test}}\log \left(\hat{\pi}_0(\bx_i)\mathbbm{1}_0(N_i) + 
	(1-\hat{\pi}_0(\bx_i))\frac{e^{-\hat{\lambda}(\bx_i)}\hat{\lambda}(\bx_i)^{N_i}}{N_i!}\right),
\end{equation} 
where $\hat{\pi}_0$ and $\hat{\lambda}$ are estimated on the learning data. 
Since we know the true underlying model, we define another two test metrics for $\pi$ and $\lambda$ in terms of $F$ and $G$, respectively:
\begin{equation}\label{t1}
	e_\pi=\frac{1}{|\mathcal{I}_\text{test}|}\sum_{i\in\mathcal{I}_\text{test}}(\hat{F}(\bx_i)-F(\bx_i))^2,
\end{equation}
\begin{equation}\label{t2}
	e_\lambda=\frac{1}{|\mathcal{I}_\text{test}|}\sum_{i\in\mathcal{I}_\text{test}}(\hat{G}(\bx_i)-G(\bx_i))^2.
\end{equation}


We start with a null ZIP model without any covariates:
\begin{equation}\label{zip-null}
	F(\bx)=\alpha_0, ~~G(\bx)=\beta_0. 
\end{equation}
The MLE is obtained as $\bar{\pi}_0=0.5604, \bar{\lambda}=0.4860$ via the EM algorithm on the learning data.
The test loss is calculated as $0.5299$. 
The estimated proportion of zero claims for the test data is calculated as 
$$\frac{1}{|\mathcal{I}_\text{test}|}\sum_{i\in\mathcal{I}_\text{test}}\left(\bar{\pi}_0+(1-\bar{\pi}_0)e^{-\bar{\lambda}}\right)=\bar{\pi}_0+(1-\bar{\pi}_0)e^{-\bar{\lambda}}=0.8308,$$
which is quite close to the empirical proportion of 83.10\%.
Since we know the true underlying $\pi_0(\bx)$ and $\lambda(\bx)$, we can calculate the true test loss as 
\begin{equation}
	-\frac{1}{|\mathcal{I}_\text{test}|}\sum_{i\in\mathcal{I}_\text{test}}\log \left({\pi}_0(\bx_i)\mathbbm{1}_0(N_i) + 
	(1-{\pi}_0(\bx_i))\frac{e^{-{\lambda}(\bx_i)}{\lambda}(\bx_i)^{N_i}}{N_i!}\right)=0.5150,
\end{equation}   
which is (surely) smaller than 0.5299 from the null model.
The test metrics \eqref{t1} and \eqref{t2} are calculated as $e_\pi=0.1151, e_\lambda=0.1795$.

Next ZIP models with linear predictors are fitted, where $F(x)$ and $G(x)$ are assumed to be linear functions of $\bx$.
One component model degenerates into a probability mass 1 at $N=0$, so there is no coefficient in this component model. 
We consider three different mixture models,  heterogeneous Poisson means only  \eqref{zip-cp},  heterogeneous mixing probabilities only \eqref{zip-cl} and both heterogeneous  mixing probabilities and Poisson means \eqref{zip-v}:
\begin{align}
	F(\bx)&=\alpha_0, ~~G(\bx)=\langle \boldsymbol{\beta}, \bx\rangle, \label{zip-cp} \\
	F(\bx)&=\langle \boldsymbol{\alpha},\bx\rangle, ~~G(\bx)=\beta_0, \label{zip-cl} \\
	F(\bx)&=\langle \boldsymbol{\alpha},\bx \rangle, ~~G(\bx)=\langle \boldsymbol{\beta}, \bx \rangle, \label{zip-v}
\end{align}  
where $\langle \boldsymbol{\beta}, \bx \rangle$ denotes the dot product of two vectors $\boldsymbol{\beta}, \bx$. The MLE of the regression coefficients $\boldsymbol{\alpha}$ and $\boldsymbol{\beta}$ can be obtained by the EM algorithm.
We have summarized the results in Table \ref{zip}.
Model \eqref{zip-v} has the best out-of-sample performance followed by Model \eqref{zip-cp}, while Model \eqref{zip-cl} has the worst out-of-sample performance.
All three models are better than the null model \eqref{zip-null}.
Note that those models ignore the potential non-linearity and interactions in covariates.

Finally, we estimate $F$ and $G$ non-parametrically via the proposed EB algorithm. We also consider three different mixture models, heterogeneous Poisson means only  \eqref{zip-bst-cp}, heterogeneous mixing probabilities only \eqref{zip-bst-cl} and both heterogeneous mixing probabilities and Poisson means \eqref{zip-bst-v}: 
\begin{align}
	F(\bx)&=\alpha_0, ~~G(\bx)=G^{[0]}(\bx)+\sum_{m=1}^{M_\lambda} s_\lambda\omega_\lambda^{[m]} g^{[m]}(\bx), \label{zip-bst-cp} \\
	F(\bx)&=F^{[0]}(\bx)+\sum_{m=1}^{M_\pi} s_\pi\omega_\pi^{[m]} f^{[m]}(\bx), ~~G(\bx)=\beta_0, \label{zip-bst-cl} \\
	F(\bx)&=F^{[0]}(\bx)+\sum_{m=1}^{M_\pi} s_\pi\omega_\pi^{[m]} f^{[m]}(\bx),  ~~G(\bx)=G_0(\bx)+\sum_{m=1}^{M_\lambda} s_\lambda\omega_\lambda^{[m]} g^{[m]}(\bx). \label{zip-bst-v}
\end{align}  
The results are listed in Table \ref{zip}.
Although the test losses \eqref{zip-test-loss} change slightly among different models, we can clearly order the considered models according to the other two metrics $e_\pi$ \eqref{t1} and $e_\lambda$ \eqref{t2}.
We observe that Model \eqref{zip-bst-v} has the best out-of-sample performance in terms of all the metrics.
Furthermore, all the models can address the excess of zeros since they are all based on the ZIP distribution.
We conclude that when there is a complicated effect of covariates on parameters, the EB algorithm is more appropriate than the EM algorithm since more flexible regression functions are allowed in the EB algorithm.   
\begin{table}[htp!]
	\caption{Comparison of different ZIP models on the test data. Note that Models \eqref{zip-null}, \eqref{zip-cp}, \eqref{zip-cl} and \eqref{zip-v} are estimated via the EM algorithm, while Models \eqref{zip-bst-cp}, \eqref{zip-bst-cl} and \eqref{zip-bst-v} are estimated via the EB algorithm.} \label{zip}
	\centering
	\begin{tabular}{ccccc}
		
		\hline\hline
		model   & test loss \eqref{zip-test-loss}  & $e_\pi$ \eqref{t1}& $e_\lambda$ \eqref{t2} & proportion of zero claims \\ \hline
		Null \eqref{zip-null}    & 0.5299 & 0.1151     & 0.1795     & 0.8308        \\
		Varying $\lambda$ \eqref{zip-cp} & 0.5258 & 0.1112     & 0.1654     & 0.8305        \\
		Varying $\pi$ \eqref{zip-cl} & 0.5265 & 0.0959     & 0.182      & 0.8304        \\
		Varying $\pi$ and $\lambda$  \eqref{zip-v} & 0.5257 & 0.1159     & 0.1776     & 0.8304        \\
		\hline
		Varying $\lambda$  \eqref{zip-bst-cp} & 0.5234 & 0.1114     & 0.0928     & 0.8316        \\
		Varying $\pi$   \eqref{zip-bst-cl} & 0.5256 & 0.0579     & 0.1738     & 0.8293        \\
		Varying $\pi$ and $\lambda$  \eqref{zip-bst-v}  & 0.5199 & 0.0687     & 0.0588     & 0.8336        \\
		\hline
		True    & 0.5150  & 0.0000         & 0.0000         & 0.8310         \\ \hline\hline
	\end{tabular}
\end{table}

{\bf Remarks.}
In the case of $K=2$, there is only one to-be-boosted function $F$ in mixing probabilities. 
Here, we replace the boosting loop (lines 5-12) in Algorithm \ref{EB} by the $L_2$\_TreeBoost algorithm in \citet{friedman2001greedy},
which boosts only one function $F$.   
Indeed, we might still apply the boosting loop (lines 5-12), which boosts two functions $F_1$ and $F_2$. The difference between $F_1$ and $F_2$ is $F$, i.e., $F_1-F_2=F.$
In this example, the EB algorithm takes only several seconds since there are only two regression functions $F$ and $G$ to be estimated.
When mixture models contain $K>2$ components, the boosting of mixing probabilities takes much more time since $K$ functions are to be boosted. This will be discussed in the next example.


\subsection{Second simulated example: mixture of Gaussians}

We consider a mixture of three Gaussians. 
Suppose that the mixing probabilities are related to covariates, while the parameters of component models are homogeneous among all samples.
The probability density function is given by
\begin{equation}\label{gaussian-true}
f(Y|\bx)=p_1(\bx)f_N(Y;\mu_1,\sigma_1)+p_2(\bx)f_N(Y;\mu_2,\sigma_2)+p_3(\bx)f_N(Y;\mu_3,\sigma_3),
\end{equation}
where $f_N(\cdot;\mu,\sigma)$ is the Gaussian probability density function with mean $\mu$ and standard deviation $\sigma$.
We assume that the mixing probabilities depend on the covariate vector $\bx=(x_1,x_2,x_3,x_4)$ via the following equations
\begin{equation}\label{logistic-ex}
	p_k(\bx)=\frac{\exp\left(F_k(\bx)\right)}{\sum_{l=1}^{3}\exp\left(F_l(\bx)\right)}, \text{ for } k=1,2,3,
\end{equation}
where 
\begin{align*}
F_1(\bx)&=x_1+\log x_2,\\
 F_2(\bx)&=1-0.5x_1-x_1x_2+2x_3, \\
  F_3(\bx)&=2\sin x_1+\log x_2 + x_1x_3.
	\end{align*}
Note that covariate $x_4$ is a redundant variable.
% and the relationship between mixing probabilities and covariates is non-linear with interaction. 
We specify other parameters as $\mu_1=-5,\mu_2=0,\mu_3=5, \sigma_1=\sigma_2=\sigma_3=1$. We generate the  covariates from the following distributions:
   \begin{align*}
   	x_1~&\sim~ N(2,1),\\
   	x_2~&\sim~ Exp(2), \\
   	x_3~&\sim~ Bernulli(0.5), \\
   	x_4~&\sim~ \Gamma(0.5,0.5).
   \end{align*}
Note that we use shape-rate parameters for gamma distribution $\Gamma$.
We generate $n=12,000$ samples, among which $10,000$ samples are learning data and $2,000$ samples are test data.

First, we  consider the parametric mixture models calibrated by the EM algorithm. The following mixtures of three linear regressions (or distributions) are fitted:
\begin{itemize}
	\item 	Mixture of three distributions with homogeneous mixing probabilities (null model without any covariates):
	\begin{equation}\label{gaussian-0}
		f(Y|\bx)=p_1f_N(Y;\mu_1,\sigma_1)+p_2f_N(Y;\mu_2,\sigma_2)+p_3f_N(Y;\mu_3,\sigma_3).
	\end{equation}

	\item 	Mixture of three linear regressions with homogeneous mixing probabilities:
\begin{equation}\label{gaussian-glm-mu}
	f(Y|\bx)=p_1f_N(Y;\mu_1(\bx),\sigma_1)+p_2f_N(Y;\mu_2(\bx),\sigma_2)+p_3f_N(Y;\mu_3(\bx),\sigma_3),
\end{equation}
where
\begin{equation}\label{linear-mu}
	\mu_1(\bx)=\langle\boldsymbol{\alpha},\bx\rangle, \mu_2(\bx)=\langle\boldsymbol{\beta},\bx\rangle,\mu_3(\bx)=\langle\boldsymbol{\gamma},\bx\rangle.
\end{equation}

\item 	Mixture of three distributions with heterogeneous mixing probabilities modeled by a multiclass logistic regression:
\begin{equation}\label{gaussian-glm-p}
	f(Y|\bx)=p_1(\bx)f_N(Y;\mu_1,\sigma_1)+p_2(\bx)f_N(Y;\mu_2,\sigma_2)+p_3(\bx)f_N(Y;\mu_3,\sigma_3)
\end{equation}
where
\begin{equation}\label{linear-p}
	p_k(\bx)=\frac{\exp\left(F_k(\bx)\right)}{\sum_{l=1}^{3}\exp\left(F_l(\bx)\right)}, \text{ for } k=1,2,3,
\end{equation}
and 
\begin{equation}\label{linear-p2}
	F_1(\bx)=\langle \boldsymbol{a},\bx\rangle, F_2(\bx)=\langle \boldsymbol{b},\bx\rangle,F_3(\bx)=\langle \boldsymbol{c},\bx\rangle.
\end{equation}
Note that this model follows the underlying mixture model. 
However, this model cannot address the non-linearity and interaction of covariates on the mixing probabilities.

	\item 	Mixture of three linear regressions with both heterogeneous means and heterogeneous mixing probabilities:
	\begin{equation}\label{gaussian-glm-b}
		f(Y|\bx)=p_1(\bx)f_N(Y;\mu_1(\bx),\sigma_1)+p_2(\bx)f_N(Y;\mu_2(\bx),\sigma_2)+p_3(\bx)f_N(Y;\mu_3(\bx),\sigma_3)
	\end{equation}
where the means and the mixing probabilities follow equations \eqref{linear-mu}, \eqref{linear-p} and \eqref{linear-p2}.
\end{itemize}

Next, we consider non-parametric mixture models calibrated by the proposed EB algorithm. The following mixture models are fitted:
\begin{itemize}
\item Mixture model with heterogeneous means and homogeneous mixing probabilities:
\begin{equation}\label{gaussian-bst-mu}
	f(Y|\bx)=p_1f_N(Y;\mu_1(\bx),\sigma_1)+p_2f_N(Y;\mu_2(\bx),\sigma_2)+p_3f_N(Y;\mu_3(\bx),\sigma_3),
\end{equation}
where $\mu_1,\mu_2,\mu_3$ are estimated non-parametrically via the boosting algorithm. This model is comparable to model \eqref{gaussian-glm-mu}.

\item Mixture model with heterogeneous mixing probabilities:
\begin{equation}\label{gaussian-bst-p}
	f(Y|\bx)=p_1(\bx)f_N(Y;\mu_1,\sigma_1)+p_2(\bx)f_N(Y;\mu_2,\sigma_2)+p_3(\bx)f_N(Y;\mu_3,\sigma_3)
\end{equation}
where
\begin{equation}\label{bst-p}
	p_k(\bx)=\frac{\exp\left(F_k(\bx)\right)}{\sum_{l=1}^{3}\exp\left(F_l(\bx)\right)}, \text{ for } k=1,2,3.
\end{equation}
Here, $F_1,F_2,F_3$ are estimated non-parametrically via the boosting algorithm. This model is comparable to model \eqref{gaussian-glm-p}.

\item Mixture model with both heterogeneous means and heterogeneous mixing probabilities:
\begin{equation}\label{gaussian-bst-b}
	f(Y|\bx)=p_1(\bx)f_N(Y;\mu_1(\bx),\sigma_1)+p_2(\bx)f_N(Y;\mu_2(\bx),\sigma_2)+p_3(\bx)f_N(Y;\mu_3(\bx),\sigma_3)
\end{equation}
where both means and mixing probabilities are estimated non-parametrically via the boosting algorithm. This model is comparable to model \eqref{gaussian-glm-b}.
\end{itemize}

We summarize the results of the seven models in Table \ref{gaussian-summary} including average negative log-likelihood on the test data and the running time of the EM algorithm or the EB algorithm. 
Note that the true negative log-likelihood is calculated based on the true underlying model (i.e., without estimation error).
All three models with linear predictors  \eqref{gaussian-glm-mu}, \eqref{gaussian-glm-p} and \eqref{gaussian-glm-b} have a larger test loss 
than their counterpart non-parametric models \eqref{gaussian-bst-mu}, \eqref{gaussian-bst-p} and \eqref{gaussian-bst-b}, respectively,
since they cannot capture non-linear effects and interactions of covariates.
%Thus, when there are complicated effects of covariates on the parameters, the EB algorithm is better than the EM algorithm.
The two models with both heterogeneous means and mixing probabilities \eqref{gaussian-glm-b} and \eqref{gaussian-bst-b}  have no evident better out-of-sample performance than the two models with only heterogeneous probabilities \eqref{gaussian-glm-p} and \eqref{gaussian-bst-p}, respectively,
indicating that the component means are not entirely related to the covariates.
\begin{table}[htp!]
	\caption{Comparison of different models in terms of test loss and running time. Note that Models \eqref{gaussian-0}, \eqref{gaussian-glm-mu}, \eqref{gaussian-glm-p} and \eqref{gaussian-glm-b} are estimated via the EM algorithm, while Models \eqref{gaussian-bst-mu}, \eqref{gaussian-bst-p} and \eqref{gaussian-bst-b} are estimated via the EB algorithm.}\label{gaussian-summary}
	\centering
	\begin{tabular}{c|ccc}
		\hline\hline
		\multirow{2}{*}{model} & test       & running time               & running time            \\
		& loss & without parallel computing & with parallel computing \\ \hline
		Null \eqref{gaussian-0}                   & 2.4821         & $<$ 10 seconds       & NA    \\
		Varying $\mu$	\eqref{gaussian-glm-mu}                & 2.4779         & $<$ 10 seconds       & NA    \\
		Varying  $p$ \eqref{gaussian-glm-p}                 & 2.2016         & $<$ 10 seconds       & NA   \\
		Varying $\mu$ and $p$ \eqref{gaussian-glm-b}              & 2.2011         & $<$ 10 seconds       & NA   \\
		\hline
		Varying $\mu$ \eqref{gaussian-bst-mu}                & 2.4139         & $<$ 10 seconds       & NA    \\
		Varying $p$ \eqref{gaussian-bst-p}                 & 2.1330          &            650 seconds                & 381 seconds           \\
		Varying $\mu$ and $p$ \eqref{gaussian-bst-b}              & 2.1320          &              1054 seconds              & 597 seconds               \\
		\hline
		True                   & 2.1231         &            NA                &               NA         \\ \hline\hline
	\end{tabular}
\end{table}

For Model \eqref{gaussian-bst-b}, we draw the trace plot of the loss during the EB iterations $t=1,\ldots,T$ in Figure \ref{gaussian-trace}.
It shows that the EB algorithm converges after around 10 iterations. 
At the last EB iteration $T$, we draw the trace plot of the loss during the boosting iterations $m=1,\ldots,M_0$ for mixing probabilities in Figure \ref{gaussian-trace}.
It shows that the boosting is robust against overfitting.
At the last EB iteration $T$, we also draw the trace plot of the loss during the boosting iterations $m=1,\ldots,M_k$ for each component mean in Figure \ref{gaussian-comp}.
It shows that the boosting stops early at the very beginning  without finding heterogeneity in component means. 
We calculate the overall relative importance (scaled to sum 100) of $x_1,x_2,x_3,x_4$ in the mixing probabilities $(IP_j)_{j=1:4}$ as $48.23,34.51,17.00,0.25$, which aligns with the underlying model.

\begin{figure}[htp!]
		\centering
	\includegraphics[width=0.4\linewidth]{../plots/three_gaussians/loss-bst-both}
		\includegraphics[width=0.4\linewidth]{../plots/three_gaussians/loss-bst-both-p}
\caption{Left: The trace plot of loss during the EB iterations. Right: The trace plot of loss during the boosting iterations for mixing probabilities at last EB iteration $T$.}\label{gaussian-trace}
\end{figure} 

\begin{figure}[htp!]
	\centering
	\includegraphics[width=0.3\linewidth]{../plots/three_gaussians/loss-bst-both-1}
	\includegraphics[width=0.3\linewidth]{../plots/three_gaussians/loss-bst-both-2}
	\includegraphics[width=0.3\linewidth]{../plots/three_gaussians/loss-bst-both-3}
	\caption{The trace plots of loss during the boosting iterations for component means  at last EB iteration $T$.}\label{gaussian-comp}
\end{figure} 
%while the two models with only heterogeneous mixing probabilities \eqref{gaussian-glm-p} and \eqref{gaussian-bst-p} have a much better out-of-sample performance than the null model \eqref{gaussian-0}.

%Thus, we conclude that an appropriate mixing structure is heterogeneous mixing probabilities with homogeneous means. 


Comparison of the running times for models \eqref{gaussian-bst-mu} and \eqref{gaussian-bst-p} without parallel computing implies that 
the boosting of the mixing probabilities takes much more time than the boosting of the component means.
One reason is that the boosting of component means stops early while the boosting of mixing probabilities takes more time to discover the heterogeneity; see Figures \ref{gaussian-trace} and \ref{gaussian-comp}. 
We accelerate the EB algorithm by applying parallel computation in the boosting of mixing probabilities, i.e., in lines 6-9 of Algorithm \ref{EB} we fit $K$ trees simultaneously.
The last few lines in Table \ref{gaussian-summary} show  that parallel computing accelerates the EB algorithm significantly.   


%In the EB algorithm, we need to determine when to stop inner boosting loop and when to stop outer EB loop.
%We further partition the learning data into training data and validation data. 
%We early stop the inner boosting iteration according to validation loss and stop the outer EB iteration according to training loss. Early stop of the inner boosting iteration can avoid overfitting in each EB iteration, while the outer EB iteration should stop at the convergence of the training loss.


\subsection{Real data example: claims severity modeling}\label{sec:third example}

In this example, we study the claims amount data {\tt freMTPL2sev} from R package {\it CASdatasets}, which contains risk features and  claim amounts from $n=24,938$ motor third-part liabilities policies (after data cleaning). 
The claims severities are calculated as the ratio of the totoal claims amount to the number of claims for each policy.

There are 9 risk features available. 
The location-related features include the region in France {\tt Reg}, the density of the city community {\tt Area} and the density of inhabitants {\tt Dens}.
The vehicle-related features include the power of the car {\tt VehP}, the vehicle age {\tt VehA}, the vehicle brand {\tt VehB} and the car gas {\tt VehG}.
The driver-related feature is the driver age {\tt DrivA}. 
The claims-history-related feature is the bonus-malus score  {\tt BM}.  
In the boosting algorithm, we replace the two categorical variables {\tt Reg} and {\tt VehB} by means of claims severities in each categorical level.
There is a strong linearity between {\tt Area} and {\tt Dens} and a heavy tail in {\tt Dens}. 
In the GLM, we typically need to remove one of {\tt Area} and {\tt Dens} and take the logarithm of {\tt Dens}.
However, there is no need for such feature engineering in the boosting.

We plot the histogram and the cumulative distribution function of logged claims severities in Figure \ref{hist}, which shows three peaks and a heavy tail. 
An intuitive choice of component distributions is that 
three peaks are modeled by three gamma distributions, the resting non-tail part by a fourth gamma distribution, and the tail part by a Pareto distribution.
Therefore, the probability density function (of the null model) is given by
\begin{equation}\label{sev-0}
	f(Y|\bx)=\sum_{k=1}^4p_kf_{G}(Y;\mu_k,\phi_k)+p_5f_{P}(Y;\alpha,M),
\end{equation}
	where $\mu,\phi$ are the mean and dispersion parameters of gamma distribution, and $\alpha, M$ are the tail index and threshold of Pareto distribution. 
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.4\linewidth]{../plots/sev/hist.png}
	\includegraphics[width=0.4\linewidth]{../plots/sev/cdf.png}
	\caption{The histogram and the cumulative distribution function of logged claims severities.}\label{hist}
\end{figure}

	The logged survival function for logged claims severities as shown in Figure \ref{tail} indicates a regularly varying  tail at infinity \citep{embrechts2013modeling}.
	The threshold of Pareto distribution is selected as $M=8158.13$ according to the Hill plot \citep{resnick1997heavy} as shown in Figure \ref{tail}. One may refer to \citet{wuthrich2022statistical} for more discussions on this data.
	
	
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.4\linewidth]{../plots/sev/log-log.png}
	\includegraphics[width=0.4\linewidth]{../plots/sev/hill.png}
	\caption{The logged survival function and the Hill plot for logged claims severities.}\label{tail}
\end{figure}
	
	The three peaks as shown in Figure \ref{hist} imply a way of initialization through the latent component indicator variable:
	\begin{equation}
		\begin{aligned}
			\hat{\bz}^{[0]}_i&=(\hat{z}^{[0]}_{i,1},\hat{z}^{[0]}_{i,2},\hat{z}^{[0]}_{i,3},\hat{z}^{[0]}_{i,4},\hat{z}^{[0]}_{i,5})\\
			&=(\mathbbm{1}_{(0,500]}y_i,\mathbbm{1}_{(500, 1000]}y_i,\mathbbm{1}_{(1000,1200]}y_i,\mathbbm{1}_{(1200,8158.13]}y_i,\mathbbm{1}_{(8158.13,\infty)}y_i).
		\end{aligned}
	\end{equation}
Given the latent component indicator variable $\hat{\bz}^{[0]}_{i=1:n}$, the parameters $p,\mu,\phi,\alpha$ can be initialized as the MLEs in the homogeneous model on the full information $(y_i,\hat{\bz}_i^{[0]},\bx_i)_{i=1:n}$.
%, i.e., the MLEs of parameters in gamma/Pareto distribution fitted to the partial samples in five intervals $(0,500], (500,1000], (1000,1200],(1200,8158.13],(8158.13,\infty)$, respectively.
We split the data into a training data set, a validation data set and a test data set by a ratio of 3:1:1. 
	
We first fit a mixture of distributions \eqref{sev-0} to the data via the EM algorithm. 
%The trace of learning loss is shown in Figure \ref{null_sev}, implying a convergence after 80 iterations.
%	\begin{figure}[h!]
%		\centering
%		\includegraphics[width=0.4\linewidth]{../plots/sev/null_trace}
%		\caption{The trace plot the learning loss for the homogeneous model \eqref{sev-0} during the EM iterations.}\label{null_sev}
%	\end{figure}
The estimated component parameters are listed in Table \ref{null-gamma}. We observe that the three peaks are captured by the first three component gamma distributions.
	\begin{table}[h!]
		\centering
		\caption{The MLEs of component parameters in the homogeneous model \eqref{sev-0}. Tail index is estimated as $\hat{\alpha}=1.0773$.}\label{null-gamma}
		\begin{tabular}{crrrr}
			\hline
			component $k$ & \multicolumn{1}{c}{$\mu_k$} & \multicolumn{1}{c}{shape $(1/\phi_k)$} & \multicolumn{1}{c}{scale} & \multicolumn{1}{c}{rate} \\ \hline
			1         & 76.8727                & 105.556                   & 0.7283                    & 1.3731                   \\
			2         & 592.5909               & 653.539                   & 0.9067                    & 1.1029                   \\
			3         & 1171.3811              & 999.9999                  & 1.1714                    & 0.8537                   \\
			4         & 1534.5143              & 1.0377                    & 1478.7768                 & 7e-04                    \\ \hline
		\end{tabular}
	\end{table}
	The first three large shape parameters (small dispersion) imply the  difficulties with {mean modeling} in the first three component models.
	 The test loss is calculated as 7.5815. The mixing probabilities are estimated as $\hat{p}_1=0.0409,~ \hat{p}_2=0.0300, ~\hat{p}_3=0.4100,~ \hat{p}_4=0.4973$ and $\hat{p}_5=0.0218.$

Next we fit a mixture of distributions with heterogeneous mixing probabilities to the data via the proposed EB algorithm:
\begin{equation}\label{sev-bst-p}
f(y|\bx;p,\mu,\phi,\alpha)=\sum_{k=1}^4p_k(\bx)f_{G}(y;\mu_k,\phi_k)+p_5(\bx)f_{P}(y;\alpha,M).
\end{equation}
For the last EB iteration, we draw the trace plot of the multiclass logistic loss during the boosting of mixing probabilities in Figure \ref{bx-bst-p}, which shows that the boosting is quite robust against overfitting.
We draw the boxplot of the estimated mixing probabilities in Figure \ref{bx-bst-p}.
We observe that the mixing probabilities for the third and forth components $p_3$ and $p_4$ are more related with the covariates than $p_1,p_2,p_5$.
The test loss is  7.5580, smaller than the null model.
	\begin{figure}[htp!]
		\centering
				\includegraphics[width=0.4\linewidth]{../plots/sev/bst_p_trace}
		\includegraphics[width=0.4\linewidth]{../plots/sev/bst_p}
		\caption{Left: The trace plot of the loss during the boosting of mixing probabilities. Right: The boxplot of the estimated mixing probabilities in Model \eqref{sev-bst-p}.}\label{bx-bst-p}
	\end{figure}

We calculate the relative importance of risk features in each mixing probability, respectively. 
The unscaled relative importance $(IP_{j,k})_{j=1:9,k=1:5}$ shown in Figure \ref{sev-var-imp} indicates that the risk features have much stronger systematic effects on $p_3,p_4$ than $p_1,p_2,p_5$, which is consistent with the observation in Figure \ref{bx-bst-p}.
The overall relative importance $(IP_{j})_{j=1:9}$ is shown in the last plot of Figure \ref{sev-var-imp}.
The most important risk features in predicting claims severity are the bonus-malus score and the driver age.   
The vehicle age and brand are two important vehicle-related features, while the vehicle power and gas are not important in predicting claims severity.
All region-related risk features are important. Since {\tt Area} has a strong colinearity with {\tt Dens}, we do not need the first risk feature if the second one is in the model.
	\begin{figure}[htp!]
	\centering

	\includegraphics[width=0.4\linewidth]{../plots/sev/sev1}
	\includegraphics[width=0.4\linewidth]{../plots/sev/sev2}
	\includegraphics[width=0.4\linewidth]{../plots/sev/sev3}
	\includegraphics[width=0.4\linewidth]{../plots/sev/sev4}
		\includegraphics[width=0.4\linewidth]{../plots/sev/sev5}
							\includegraphics[width=0.4\linewidth]{../plots/sev/sev0}
	\caption{The relative importance of risk features in the mixing probabilities.}\label{sev-var-imp}
\end{figure}

%Finally, we observe that the small shape parameter in the forth component as shown in Table \ref{null-gamma} might indicates a possible improvement by boosting the forth component  mean.
%Therefore, we fit the following mixture model:
%	$$
%	\begin{aligned}
%		f(y|\bx;p,\mu,\phi,\alpha)=&\sum_{k=1}^3p_k(\bx)f_{G}(y;\mu_k,\phi_k)+ \\
%		&p_4(\bx)f_{G}(y|\bx;\mu_4(\bx),\phi_4)+ p_5(\bx)f_{P}(y;\alpha,M).
%	\end{aligned}
%	$$
%	The test loss is calculated as 7.5573 slightly smaller than that for model \eqref{sev-bst-p}. 
%For this model, we calculate the relative importance of covariates.


\section{Conclusions}\label{sec:conclusions}
Insurance loss data sometimes cannot be sufficiently modeled by a single distribution, 
so a mixture model is usually more preferred. 
Since there are multiple component models in a mixture model, 
the parameter estimation problem is one key issue for data analysis. 
Thus, the EM algorithm is one critical estimation method for mixture models in the literature.
However, the EM algorithm usually requires a parametric form of the component models, which limits the practical flexibility of mixture models.
In this paper, we propose an Expectation-Boosting (EB) algorithm for mixture models, 
where both the mixing probabilities and the component models can be modeled non-parametrically. 

The core of the proposed algorithm is to replace the maximization step of the EM algorithm with a generic functional gradient descent algorithm.
There are several advantages of the EB algorithm over the EM algorithm. 
First, boosting algorithm is a flexible non-parametric regression facilitating both {non-linear effects and interaction}.  
There is no need to specify the form of component regression functions.
Automated feature engineering is performed during the EB algorithm.
Second, boosting algorithm is {overfitting-sensitive}, and {variable selection} is conducted simultaneously during the EB algorithm.
The proposed algorithm requires more computing time for modeling flexibility than the EM algorithm since the boosting is nested inside each EB iteration.
We propose accelerating the EB algorithm by parallel computing since there are some independent steps or loops at each EB iteration given the component indicator variable $\bz$.
%In this paper, although we do not discuss model interpretation, the interpretation method in usual boosting algorithm can be implemented directly for mixture models such as variable importance, partial dependent plot, etc.

	\section*{Acknowledgment}
Guangyuan Gao gratefully acknowledges the financial support from the National Natural Science Foundation of China (71901207). Yanxi Hou gratefully acknowledges the financial support from the National Natural Science Foundation of China (72171055, 71991471) and Natural Science Foundation of Shanghai (20ZR1403900).

\bibliography{boosting}
\bibliographystyle{boosting}

\end{document}
