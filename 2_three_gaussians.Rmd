---
title: "Gaussian Mixture Boosting"
author: "Jiahong Li"
date: "2021/9/16"
output: html_document
editor_options: 
  chunk_output_type: console
---

# negative log-likelihood

```{r}
rm(list=ls())
source("0_multiclass_bst.R")
source("0_gaussian_bst.R")
```

# Data genaration

```{r data generation}
names(dat)

# png("./plots/boxplot_p.png")
boxplot(dat[,c("P1","P2","P3")],main="boxplot of mixing probabilities")
# dev.off()

boxplot(dat_test[,c("P1","P2","P3")])
boxplot(dat[,c("Y1","Y2","Y3")])
boxplot(dat_test[,c("Y1","Y2","Y3")])

# png("./plots/histogram.png")
plot(density(dat$Y,width = 2),main="histogram of Y",xlab="Y")
abline(v=c(-8,0,8),lty=2)
# dev.off()
```

# Homogenous model

```{r}
X<-dat[, c("X1","X2","X3","X4")]
Y<-dat$Y
Xtest<-dat_test[, c("X1","X2","X3","X4")]
Ytest<-dat_test$Y
M0<-50
model<-"null"
structure<-"null"
trace<-T
patience<-3
model_homo<-EM_gaussian(X=X, Y=Y, Xtest=Xtest, Ytest=Ytest, M0=M0, model=model, structure=structure, trace=trace, patience=patience)

ylim0<-c(2.85,3.35)
# png("./plots/loss-null.png")
matplot(cbind(model_homo$learn_loss,model_homo$test_loss), lty = c(1,1) ,col=c('red',"blue"),ylim=ylim0,xlab = "iterations of EM",ylab="neg LL",type="l", main="EM algorithm for null model")
legend("topright",c("learn loss","test loss"),col=c("red","blue"),lty=c(1,1))
abline(v=which.min(model_homo$learn_loss),lty=2,col="red")
abline(v=which.min(model_homo$test_loss),lty=2,col="blue")
# dev.off()
which.min(model_homo$test_loss)

(homo_learn_loss <- min(model_homo$learn_loss))
(homo_test_loss <- min(model_homo$test_loss))
(true_test_loss<-neg_ll3(dat_test$Y,dat_test[,c("MU1","MU2","MU3")],dat_test[,c("SG1","SG2","SG3")], dat_test[,c("P1","P2","P3")]))

(eH<-c(mean((dat_test$MU1-model_homo$test$mu1)^2),mean((dat_test$MU2-model_homo$test$mu2)^2),mean((dat_test$MU3-model_homo$test$mu3)^2),mean((dat_test$F1-model_homo$test$plinear1)^2), mean((dat_test$F2-model_homo$test$plinear2)^2), mean((dat_test$F3-model_homo$test$plinear3)^2) ))
# head(model_homo$valid)
```

# GLMs 

## with varying mu

```{r}
X<-dat[,c("X1","X2","X3","X4")]
Y<-dat$Y
Xtest<-dat_test[,c("X1","X2","X3","X4")]
Ytest<-dat_test$Y
M0<-60
model<-"glm"
structure<-"mu"
trace<-T
patience<-3
model_glm_mu<-EM_gaussian(X=X, Y=Y, Xtest=Xtest, Ytest = Ytest, M0=M0, model=model, structure=structure, trace=trace,patience = patience)

# png("./plots/loss-glm-mu.png")
matplot(cbind(model_glm_mu$learn_loss,model_glm_mu$test_loss), lty = c(1,1) ,col=c('red',"blue"),ylim=ylim0,xlab = "iterations of EM",ylab="neg LL",type="l", main="EM algorithm for linear model of mu")
legend("topright",c("learn loss","test loss"),col=c("red","blue"),lty=c(1,1))
abline(v=which.min(model_glm_mu$learn_loss),lty=2,col="red")
abline(v=which.min(model_glm_mu$test_loss),lty=2,col="blue")
# dev.off()
which.min(model_glm_mu$test_loss)

(glm_learn_loss_mu <- min(model_glm_mu$learn_loss))
(glm_test_loss_mu <- min(model_glm_mu$test_loss))
homo_test_loss
true_test_loss

(eGLM_mu<-c(mean((dat_test$MU1-model_glm_mu$test$mu1)^2),mean((dat_test$MU2-model_glm_mu$test$mu2)^2),mean((dat_test$MU3-model_glm_mu$test$mu3)^2),mean((dat_test$F1-model_glm_mu$test$plinear1)^2), mean((dat_test$F2-model_glm_mu$test$plinear2)^2), mean((dat_test$F3-model_glm_mu$test$plinear3)^2) ))
eH
```

## with varying p

```{r}
X<-dat[,c("X1","X2","X3","X4")]
Y<-dat$Y
Xtest<-dat_test[,c("X1","X2","X3","X4")]
Ytest<-dat_test$Y
M0<-50
model<-"glm"
structure<-"p"
trace<-T
patience<-5
model_glm_p<-EM_gaussian(X=X, Y=Y, Xtest=Xtest, Ytest=Ytest, M0=M0, model=model, structure=structure, trace=trace,patience=patience)

# png("./plots/loss-glm-p.png")
matplot(cbind(model_glm_p$learn_loss,model_glm_p$test_loss), lty = c(1,1) ,col=c('red',"blue"),ylim=ylim0,xlab = "iterations of EM",ylab="neg LL",type="l", main="EM algorithm for linear model of p")
legend("topright",c("learn loss","test loss"),col=c("red","blue"),lty=c(1,1))
abline(v=which.min(model_glm_p$learn_loss),lty=2,col="red")
abline(v=which.min(model_glm_p$test_loss),lty=2,col="blue")
# dev.off()
which.min(model_glm_p$valid_loss)

(glm_learn_loss_p <- min(model_glm_p$learn_loss))
(glm_test_loss_p <- min(model_glm_p$test_loss))
glm_test_loss_mu
homo_test_loss
true_test_loss

(eGLM_p<-c(mean((dat_test$MU1-model_glm_p$test$mu1)^2),mean((dat_test$MU2-model_glm_p$test$mu2)^2),mean((dat_test$MU3-model_glm_p$test$mu3)^2),mean((dat_test$F1-model_glm_p$test$plinear1)^2), mean((dat_test$F2-model_glm_p$test$plinear2)^2), mean((dat_test$F3-model_glm_p$test$plinear3)^2) ))
eGLM_mu
eH
```

## with both

```{r}
X<-dat[,c("X1","X2","X3","X4")]
Y<-dat$Y
Xtest<-dat_test[,c("X1","X2","X3","X4")]
Ytest<-dat_test$Y
M0<-60
model<-"glm"
structure<-"both"
trace<-T
patience<-5
model_glm_both<-EM_gaussian(X=X, Y=Y, Xtest=Xtest, Ytest=Ytest, M0=M0, model=model, structure=structure, trace=trace,patience = patience)

# png("./plots/loss-glm-both.png")
matplot(cbind(model_glm_both$learn_loss,model_glm_both$test_loss), lty = c(1,1) ,col=c('red',"blue"),ylim=ylim0,xlab = "iterations of EM",ylab="neg LL",type="l", main="EM algorithm for linear model of both mu and p")
legend("topright",c("learn loss","test loss"),col=c("red","blue"),lty=c(1,1))
abline(v=which.min(model_glm_both$learn_loss),lty=2,col="red")
abline(v=which.min(model_glm_both$test_loss),lty=2,col="blue")
# dev.off()
which.min(model_glm_both$test_loss)

(glm_learn_loss_both <- min(model_glm_both$learn_loss))
(glm_test_loss_both <- min(model_glm_both$test_loss))
glm_test_loss_mu
glm_test_loss_p
homo_test_loss
true_test_loss

(eGLM_both<-c(mean((dat_test$MU1-model_glm_both$test$mu1)^2),mean((dat_test$MU2-model_glm_both$test$mu2)^2),mean((dat_test$MU3-model_glm_both$test$mu3)^2),mean((dat_test$F1-model_glm_both$test$plinear1)^2), mean((dat_test$F2-model_glm_both$test$plinear2)^2), mean((dat_test$F3-model_glm_both$test$plinear3)^2) ))
eGLM_mu
eGLM_p
eH
```

# Boosting 

## with varying mu

```{r}
X<-dat[1:8000,c("X1","X2","X3","X4")]
Y<-dat$Y[1:8000]
Xval<-dat[8001:10000,c("X1","X2","X3","X4")]
Yval<-dat$Y[8001:10000]
Xtest<-dat_test[,c("X1","X2","X3","X4")]
Ytest<-dat_test$Y
M0<-50
structure<-"mu"
trace<-T

model_bst_mu<-EB_gaussian(X=X, Y=Y, Xval=Xval, Yval=Yval, M0=M0,  structure=structure, trace=trace , patience = 2)

# png("./plots/loss-bst-mu.png")
matplot(cbind(model_bst_mu$train_loss,model_bst_mu$valid_loss), lty = c(1,1) ,col=c('red',"lightblue"),ylim=ylim0,xlab = "iterations of EM",ylab="neg LL",type="l", main="EM algorithm for boosting of mu")
legend("topright",c("train loss","valid loss"),col=c("red","lightblue"),lty=c(1,1))
abline(v=which.min(model_bst_mu$train_loss),lty=2,col="red")
abline(v=which.min(model_bst_mu$valid_loss),lty=2,col="lightblue")
# dev.off()

min(model_bst_mu$valid_loss)

dat_test$mu1<-predict(model_bst_mu$mu_models[[1]],newdata = dat_test, n.trees = model_bst_mu$mu_iter[1])
dat_test$mu2<-predict(model_bst_mu$mu_models[[2]],newdata = dat_test, n.trees = model_bst_mu$mu_iter[2])
dat_test$mu3<-predict(model_bst_mu$mu_models[[3]],newdata = dat_test, n.trees = model_bst_mu$mu_iter[3])
dat_test$sigma1<-unique(model_bst_mu$sigma[1])
dat_test$sigma2<-unique(model_bst_mu$sigma[2])
dat_test$sigma3<-unique(model_bst_mu$sigma[3])

dat_test[,c("p1","p2","p3")]<-predict(model_bst_mu$p_model,newdata = dat_test, type="probs")
(bst_test_loss_mu <- neg_ll3(dat_test$Y, dat_test[, c("mu1", "mu2", "mu3")], dat_test[, c("sigma1", "sigma2", "sigma3")], dat_test[, c("p1", "p2", "p3")]))
```

## with varying p

```{r}
X<-dat[1:8000,c("X1","X2","X3","X4")]
Y<-dat$Y[1:8000]
Xval<-dat[8001:10000,c("X1","X2","X3","X4")]
Yval<-dat$Y[8001:10000]
Xtest<-dat_test[,c("X1","X2","X3","X4")]
Ytest<-dat_test$Y
M0<-50
structure<-"p"
trace<-T

model_bst_p<-EB_gaussian(X=X, Y=Y, Xval=Xval, Yval=Yval, M0=M0,  structure="p", trace=trace, patience = 2)

# png("./plots/loss-bst-p.png")
matplot(cbind(model_bst_p$train_loss,model_bst_p$valid_loss), lty = c(1,1) ,col=c('red',"lightblue"),ylim=ylim0,xlab = "iterations of EM",ylab="neg LL",type="l", main="EM algorithm for boosting of p")
legend("topright",c("train loss","valid loss"),col=c("red","lightblue"),lty=c(1,1))
abline(v=which.min(model_bst_p$train_loss),lty=2,col="red")
abline(v=which.min(model_bst_p$valid_loss),lty=2,col="lightblue")
# dev.off()


dat_test$mu1<-predict(model_bst_p$mu_models[[1]],newdata = dat_test)
dat_test$mu2<-predict(model_bst_p$mu_models[[2]],newdata = dat_test)
dat_test$mu3<-predict(model_bst_p$mu_models[[3]],newdata = dat_test)
dat_test$sigma1<-unique(model_bst_p$sigma[1])
dat_test$sigma2<-unique(model_bst_p$sigma[2])
dat_test$sigma3<-unique(model_bst_p$sigma[3])
dat_test[,c("p1","p2","p3")]<-predict_BST(Xtest, model_bst_p$p_model$Tree_save, Pinit=NULL, M_best = which.min(model_bst_p$p_model$Valid_loss), lr= model_bst_p$p_model$lr)[,c("BST_p1","BST_p2","BST_p3")]

(bst_test_loss_p <- neg_ll3(dat_test$Y, dat_test[, c("mu1", "mu2", "mu3")], dat_test[, c("sigma1", "sigma2", "sigma3")], dat_test[, c("p1", "p2", "p3")]))
```

## with varying both

```{r}
X<-dat[1:8000,c("X1","X2","X3","X4")]
Y<-dat$Y[1:8000]
Xval<-dat[8001:10000,c("X1","X2","X3","X4")]
Yval<-dat$Y[8001:10000]
Xtest<-dat_test[,c("X1","X2","X3","X4")]
Ytest<-dat_test$Y
M0<-50
structure<-"both"
trace<-T

model_bst_both<-EB_gaussian(X=X, Y=Y, Xval=Xval, Yval=Yval, M0=M0,  structure="both", trace=trace, patience = 2)

# png("./plots/loss-bst-both.png")
matplot(cbind(model_bst_both$train_loss,model_bst_both$valid_loss), lty = c(1,1) ,col=c('red',"lightblue"),ylim=ylim0,xlab = "iterations of EM",ylab="neg LL",type="l", main="EM algorithm for boosting of both mu and p")
legend("topright",c("train loss","valid loss"),col=c("red","lightblue"),lty=c(1,1))
abline(v=which.min(model_bst_both$train_loss),lty=2,col="red")
abline(v=which.min(model_bst_both$valid_loss),lty=2,col="lightblue")
# dev.off()

dat_test$mu1<-predict(model_bst_both$mu_models[[1]],newdata = dat_test, n.trees = model_bst_both$mu_iter[1])
dat_test$mu2<-predict(model_bst_both$mu_models[[2]],newdata = dat_test, n.trees = model_bst_both$mu_iter[2])
dat_test$mu3<-predict(model_bst_both$mu_models[[3]],newdata = dat_test, n.trees = model_bst_both$mu_iter[3])
dat_test$sigma1<-unique(model_bst_both$sigma[1])
dat_test$sigma2<-unique(model_bst_both$sigma[2])
dat_test$sigma3<-unique(model_bst_both$sigma[3])
dat_test[,c("p1","p2","p3")]<-predict_BST(Xtest, model_bst_both$p_model$Tree_save, Pinit=NULL, M_best = which.min(model_bst_both$p_model$Valid_loss), lr= model_bst_both$p_model$lr)[,c("BST_p1","BST_p2","BST_p3")]

(bst_test_loss_both <- neg_ll3(dat_test$Y, dat_test[, c("mu1", "mu2", "mu3")], dat_test[, c("sigma1", "sigma2", "sigma3")], dat_test[, c("p1", "p2", "p3")]))
```

```{r}
loss_mat <-
  data.frame(
    model = c(
      "homo",
      "glm_mu",
      "glm_p",
      "glm_both",
      "bst_mu",
      "bst_p",
      "bst_both"
    ),
    negL = c(
      homo_test_loss,
      glm_test_loss_mu,
      glm_test_loss_p,
      glm_test_loss_both,
      bst_test_loss_mu,
      bst_test_loss_p,
      bst_test_loss_both
    )
  )
loss_mat
# write.csv(loss_mat,"./plots/gaussian_loss_mat.csv")
```

