---
title: "Gaussian Mixture Boosting"
author: "Jiahong Li"
date: "2021/9/16"
output: html_document
editor_options: 
  chunk_output_type: console
---

# negative log-likelihood

```{r}
rm(list=ls())
source("0_multiclass_bst.R")
source("0_gaussian_bst.R")
```

# Data genaration

```{r data generation}
names(dat)

# png("./plots/three_gaussians/boxplot_p.png")
boxplot(dat[,c("P1","P2","P3")],main="boxplot of mixing probabilities")
# dev.off()

boxplot(dat_test[,c("P1","P2","P3")])
boxplot(dat[,c("Y1","Y2","Y3")])
boxplot(dat_test[,c("Y1","Y2","Y3")])

# png("./plots/three_gaussians/histogram.png")
plot(density(dat$Y,width = 2),main="histogram of Y",xlab="Y")
abline(v=c(-5,0,5),lty=2)
# dev.off()
```

# Homogenous model

```{r}
X<-dat[, c("X1","X2","X3","X4")]
Y<-dat$Y
Xtest<-dat_test[, c("X1","X2","X3","X4")]
Ytest<-dat_test$Y
M0<-50
model<-"null"
structure<-"null"
trace<-T
patience<-2
time_temp<-proc.time()
model_homo<-EM_gaussian(X=X, Y=Y, Xtest=Xtest, Ytest=Ytest, M0=M0, model=model, structure=structure, trace=trace, patience=patience)
(time_null<-proc.time()-time_temp)

ylim0<-c(2.05,2.50)
# png("./plots/three_gaussians/loss-null.png")
matplot(cbind(model_homo$learn_loss,model_homo$test_loss), lty = c(1,1) ,col=c('red',"blue"),ylim=ylim0,xlab = "iterations of EM",ylab="neg LL",type="l", main="EM algorithm for null model")
legend("topright",c("learn loss","test loss"),col=c("red","blue"),lty=c(1,1))
abline(v=which.min(model_homo$learn_loss),lty=2,col="red")
abline(v=which.min(model_homo$test_loss),lty=2,col="blue")
# dev.off()
which.min(model_homo$test_loss)

(homo_learn_loss <- min(model_homo$learn_loss))
(homo_test_loss <- min(model_homo$test_loss))
(true_test_loss<-neg_ll3(dat_test$Y,dat_test[,c("MU1","MU2","MU3")],dat_test[,c("SG1","SG2","SG3")], dat_test[,c("P1","P2","P3")]))

(eH<-c(mean((dat_test$MU1-model_homo$test$mu1)^2),mean((dat_test$MU2-model_homo$test$mu2)^2),mean((dat_test$MU3-model_homo$test$mu3)^2),mean((dat_test$F1-model_homo$test$plinear1)^2), mean((dat_test$F2-model_homo$test$plinear2)^2), mean((dat_test$F3-model_homo$test$plinear3)^2) ))
# head(model_homo$valid)
unique(model_homo$fitted$mu1)
unique(model_homo$fitted$mu2)
unique(model_homo$fitted$mu3)
```

# GLMs 

## with varying mu

```{r}
X<-dat[,c("X1","X2","X3","X4")]
Y<-dat$Y
Xtest<-dat_test[,c("X1","X2","X3","X4")]
Ytest<-dat_test$Y
M0<-60
model<-"glm"
structure<-"mu"
trace<-T
patience<-2
time_temp<-proc.time()
model_glm_mu<-EM_gaussian(X=X, Y=Y, Xtest=Xtest, Ytest = Ytest, M0=M0, model=model, structure=structure, trace=trace,patience = patience)
(time_glm_mu<-proc.time()-time_temp)

# png("./plots/three_gaussians/loss-glm-mu.png")
matplot(cbind(model_glm_mu$learn_loss,model_glm_mu$test_loss), lty = c(1,1) ,col=c('red',"blue"),ylim=ylim0,xlab = "iterations of EM",ylab="neg LL",type="l", main="EM algorithm for linear model of mu")
legend("topright",c("learn loss","test loss"),col=c("red","blue"),lty=c(1,1))
abline(v=which.min(model_glm_mu$learn_loss),lty=2,col="red")
abline(v=which.min(model_glm_mu$test_loss),lty=2,col="blue")
# dev.off()
which.min(model_glm_mu$test_loss)

(glm_learn_loss_mu <- min(model_glm_mu$learn_loss))
(glm_test_loss_mu <- min(model_glm_mu$test_loss))
homo_test_loss
true_test_loss

(eGLM_mu<-c(mean((dat_test$MU1-model_glm_mu$test$mu1)^2),mean((dat_test$MU2-model_glm_mu$test$mu2)^2),mean((dat_test$MU3-model_glm_mu$test$mu3)^2),mean((dat_test$F1-model_glm_mu$test$plinear1)^2), mean((dat_test$F2-model_glm_mu$test$plinear2)^2), mean((dat_test$F3-model_glm_mu$test$plinear3)^2) ))
eH
```

## with varying p

```{r}
X<-dat[,c("X1","X2","X3","X4")]
Y<-dat$Y
Xtest<-dat_test[,c("X1","X2","X3","X4")]
Ytest<-dat_test$Y
M0<-50
model<-"glm"
structure<-"p"
trace<-T
patience<-2
time_temp<-proc.time()
model_glm_p<-EM_gaussian(X=X, Y=Y, Xtest=Xtest, Ytest=Ytest, M0=M0, model=model, structure=structure, trace=trace,patience=patience)
(time_glm_p<-proc.time()-time_temp)


# png("./plots/three_gaussians/loss-glm-p.png")
matplot(cbind(model_glm_p$learn_loss,model_glm_p$test_loss), lty = c(1,1) ,col=c('red',"blue"),ylim=ylim0,xlab = "iterations of EM",ylab="neg LL",type="l", main="EM algorithm for linear model of p")
legend("topright",c("learn loss","test loss"),col=c("red","blue"),lty=c(1,1))
abline(v=which.min(model_glm_p$learn_loss),lty=2,col="red")
abline(v=which.min(model_glm_p$test_loss),lty=2,col="blue")
# dev.off()
which.min(model_glm_p$valid_loss)

(glm_learn_loss_p <- min(model_glm_p$learn_loss))
(glm_test_loss_p <- min(model_glm_p$test_loss))
glm_test_loss_mu
homo_test_loss
true_test_loss

(eGLM_p<-c(mean((dat_test$MU1-model_glm_p$test$mu1)^2),mean((dat_test$MU2-model_glm_p$test$mu2)^2),mean((dat_test$MU3-model_glm_p$test$mu3)^2),mean((dat_test$F1-model_glm_p$test$plinear1)^2), mean((dat_test$F2-model_glm_p$test$plinear2)^2), mean((dat_test$F3-model_glm_p$test$plinear3)^2) ))
eGLM_mu
eH
```

## with both

```{r}
X<-dat[,c("X1","X2","X3","X4")]
Y<-dat$Y
Xtest<-dat_test[,c("X1","X2","X3","X4")]
Ytest<-dat_test$Y
M0<-60
model<-"glm"
structure<-"both"
trace<-T
patience<-2
time_temp<-proc.time()
model_glm_both<-EM_gaussian(X=X, Y=Y, Xtest=Xtest, Ytest=Ytest, M0=M0, model=model, structure=structure, trace=trace,patience = patience)
(time_glm_b<-proc.time()-time_temp)


# png("./plots/three_gaussians/loss-glm-both.png")
matplot(cbind(model_glm_both$learn_loss,model_glm_both$test_loss), lty = c(1,1) ,col=c('red',"blue"),ylim=ylim0,xlab = "iterations of EM",ylab="neg LL",type="l", main="EM algorithm for linear model of both mu and p")
legend("topright",c("learn loss","test loss"),col=c("red","blue"),lty=c(1,1))
abline(v=which.min(model_glm_both$learn_loss),lty=2,col="red")
abline(v=which.min(model_glm_both$test_loss),lty=2,col="blue")
# dev.off()
which.min(model_glm_both$test_loss)

(glm_learn_loss_both <- min(model_glm_both$learn_loss))
(glm_test_loss_both <- min(model_glm_both$test_loss))
glm_test_loss_mu
glm_test_loss_p
homo_test_loss
true_test_loss

(eGLM_both<-c(mean((dat_test$MU1-model_glm_both$test$mu1)^2),mean((dat_test$MU2-model_glm_both$test$mu2)^2),mean((dat_test$MU3-model_glm_both$test$mu3)^2),mean((dat_test$F1-model_glm_both$test$plinear1)^2), mean((dat_test$F2-model_glm_both$test$plinear2)^2), mean((dat_test$F3-model_glm_both$test$plinear3)^2) ))
eGLM_mu
eGLM_p
eH
```

# Boosting 

## with varying mu

```{r}
X<-dat[1:8000,c("X1","X2","X3","X4")]
Y<-dat$Y[1:8000]
Xval<-dat[8001:10000,c("X1","X2","X3","X4")]
Yval<-dat$Y[8001:10000]
Xtest<-dat_test[,c("X1","X2","X3","X4")]
Ytest<-dat_test$Y
M0<-50
structure<-"mu"
trace<-T

time_temp<-Sys.time()
time_temp2<-proc.time()
model_bst_mu<-EB_gaussian(X=X, Y=Y, Xval=Xval, Yval=Yval, M0=M0,  structure=structure, trace=trace , patience = 2, parallel=0)
(time_m<-difftime(Sys.time(),time_temp))
(time_m2<-proc.time()-time_temp2)

# png("./plots/three_gaussians/loss-bst-mu.png")
matplot(cbind(model_bst_mu$train_loss,model_bst_mu$valid_loss), lty = c(1,1) ,col=c('red',"lightblue"),ylim=ylim0,xlab = "iterations of EM",ylab="neg LL",type="l", main="EM algorithm for boosting of mu")
legend("topright",c("train loss","valid loss"),col=c("red","lightblue"),lty=c(1,1))
abline(v=which.min(model_bst_mu$train_loss),lty=2,col="red")
abline(v=which.min(model_bst_mu$valid_loss),lty=2,col="lightblue")
# dev.off()

min(model_bst_mu$valid_loss)

dat_test$mu1<-predict(model_bst_mu$mu_models[[1]],newdata = dat_test, n.trees = model_bst_mu$mu_iter[1])
dat_test$mu2<-predict(model_bst_mu$mu_models[[2]],newdata = dat_test, n.trees = model_bst_mu$mu_iter[2])
dat_test$mu3<-predict(model_bst_mu$mu_models[[3]],newdata = dat_test, n.trees = model_bst_mu$mu_iter[3])
dat_test$sigma1<-unique(model_bst_mu$sigma[1])
dat_test$sigma2<-unique(model_bst_mu$sigma[2])
dat_test$sigma3<-unique(model_bst_mu$sigma[3])

dat_test[,c("p1","p2","p3")]<-predict(model_bst_mu$p_model,newdata = dat_test, type="probs")
(bst_test_loss_mu <- neg_ll3(dat_test$Y, dat_test[, c("mu1", "mu2", "mu3")], dat_test[, c("sigma1", "sigma2", "sigma3")], dat_test[, c("p1", "p2", "p3")]))
```

## with varying p

```{r}
X<-dat[1:8000,c("X1","X2","X3","X4")]
Y<-dat$Y[1:8000]
Xval<-dat[8001:10000,c("X1","X2","X3","X4")]
Yval<-dat$Y[8001:10000]
Xtest<-dat_test[,c("X1","X2","X3","X4")]
Ytest<-dat_test$Y
M0<-50
structure<-"p"
trace<-T

time_temp<-Sys.time()
time_temp2<-proc.time()
model_bst_p<-EB_gaussian(X=X, Y=Y, Xval=Xval, Yval=Yval, M0=M0,  structure="p", trace=trace, patience = 2, parallel = 1)
(time_p<-difftime(Sys.time(),time_temp))
(time_p2<-proc.time()-time_temp2)

# png("./plots/three_gaussians/loss-bst-p.png")
matplot(cbind(model_bst_p$train_loss,model_bst_p$valid_loss), lty = c(1,1) ,col=c('red',"lightblue"),ylim=ylim0,xlab = "iterations of EM",ylab="neg LL",type="l", main="EM algorithm for boosting of p")
legend("topright",c("train loss","valid loss"),col=c("red","lightblue"),lty=c(1,1))
abline(v=which.min(model_bst_p$train_loss),lty=2,col="red")
abline(v=which.min(model_bst_p$valid_loss),lty=2,col="lightblue")
# dev.off()


dat_test$mu1<-predict(model_bst_p$mu_models[[1]],newdata = dat_test)
dat_test$mu2<-predict(model_bst_p$mu_models[[2]],newdata = dat_test)
dat_test$mu3<-predict(model_bst_p$mu_models[[3]],newdata = dat_test)
dat_test$sigma1<-unique(model_bst_p$sigma[1])
dat_test$sigma2<-unique(model_bst_p$sigma[2])
dat_test$sigma3<-unique(model_bst_p$sigma[3])
dat_test[,c("p1","p2","p3")]<-predict_BST(Xtest, model_bst_p$p_model, Pinit=NULL, M_best = which.min(model_bst_p$p_model$Valid_loss),type="response")

(bst_test_loss_p <- neg_ll3(dat_test$Y, dat_test[, c("mu1", "mu2", "mu3")], dat_test[, c("sigma1", "sigma2", "sigma3")], dat_test[, c("p1", "p2", "p3")]))
```

## with varying both

```{r}
X<-dat[1:8000,c("X1","X2","X3","X4")]
Y<-dat$Y[1:8000]
Xval<-dat[8001:10000,c("X1","X2","X3","X4")]
Yval<-dat$Y[8001:10000]
Xtest<-dat_test[,c("X1","X2","X3","X4")]
Ytest<-dat_test$Y
M0<-50
structure<-"both"
trace<-T

time_temp<-Sys.time()
time_temp2<-proc.time()
model_bst_both<-EB_gaussian(X=X, Y=Y, Xval=Xval, Yval=Yval, M0=M0,  structure="both", trace=trace, patience = 2, parallel = 1)
(time_b<-difftime(Sys.time(),time_temp))
(time_b2<-proc.time()-time_temp2)

dat_test$mu1<-predict(model_bst_both$mu_models[[1]],newdata = dat_test, n.trees = model_bst_both$mu_iter[1])
dat_test$mu2<-predict(model_bst_both$mu_models[[2]],newdata = dat_test, n.trees = model_bst_both$mu_iter[2])
dat_test$mu3<-predict(model_bst_both$mu_models[[3]],newdata = dat_test, n.trees = model_bst_both$mu_iter[3])
dat_test$sigma1<-unique(model_bst_both$sigma[1])
dat_test$sigma2<-unique(model_bst_both$sigma[2])
dat_test$sigma3<-unique(model_bst_both$sigma[3])
dat_test[,c("p1","p2","p3")]<-predict_BST(Xtest, model_bst_both$p_model, Pinit=NULL, M_best = which.min(model_bst_both$p_model$Valid_loss), type="response")

(bst_test_loss_both <- neg_ll3(dat_test$Y, dat_test[, c("mu1", "mu2", "mu3")], dat_test[, c("sigma1", "sigma2", "sigma3")], dat_test[, c("p1", "p2", "p3")]))
```

## trace of loss and variable importance

```{r}
# png("./plots/three_gaussians/loss-bst-both.png")
matplot(cbind(model_bst_both$train_loss,model_bst_both$valid_loss), lty = c(1,1) ,col=c('red',"blue"),ylim=ylim0,xlab = "EB iterations",ylab="neg LL",type="l", main="trace plot of loss in the EB algorithm")
legend("topright",c("training loss","validation loss"),col=c("red","blue"),lty=c(1,1))
#abline(v=which.min(model_bst_both$train_loss),lty=2,col="red")
#abline(v=which.min(model_bst_both$valid_loss),lty=2,col="lightblue")
# dev.off()

# png("./plots/three_gaussians/loss-bst-both-p.png")
matplot(cbind(model_bst_both$p_model$Train_loss,model_bst_both$p_model$Valid_loss),type="l",xlab="boosting iterations for p",ylab="loss",lty = c(1,1) ,col=c('red',"blue"),main="trace plot of loss for mixing probabilites")
abline(v=which.min(model_bst_both$p_model$Valid_loss),lty=2,col="lightblue")
legend("topright",c("training loss","validation loss","early stop"),col=c("red","blue","lightblue"),lty=c(1,1,2))
# dev.off()

# png("./plots/three_gaussians/loss-bst-both-1.png")
matplot(cbind(model_bst_both$mu_models[[1]]$train.error,model_bst_both$mu_models[[1]]$valid.error),type="l",xlab="boosting iterations for mu1",ylab="loss",lty = c(1,1) ,col=c('red',"blue"),main="trace plot of loss for mu1")
abline(v=which.min(model_bst_both$mu_models[[1]]$valid.error),lty=2,col="lightblue")
legend("topright",c("training loss","validation loss","early stop"),col=c("red","blue","lightblue"),lty=c(1,1,2))
# dev.off()

# png("./plots/three_gaussians/loss-bst-both-2.png")
matplot(cbind(model_bst_both$mu_models[[2]]$train.error,model_bst_both$mu_models[[2]]$valid.error),type="l",xlab="boosting iterations for mu2",ylab="loss",lty = c(1,1) ,col=c('red',"blue"),main="trace plot of loss for mu2")
abline(v=which.min(model_bst_both$mu_models[[2]]$valid.error),lty=2,col="lightblue")
legend("topright",c("training loss","validation loss","early stop"),col=c("red","blue","lightblue"),lty=c(1,1,2))
# dev.off()

# png("./plots/three_gaussians/loss-bst-both-3.png")
matplot(cbind(model_bst_both$mu_models[[3]]$train.error,model_bst_both$mu_models[[3]]$valid.error),type="l",xlab="boosting iterations for mu3",ylab="loss",lty = c(1,1) ,col=c('red',"blue"),main="trace plot of loss for mu3")
abline(v=which.min(model_bst_both$mu_models[[3]]$valid.error),lty=2,col="lightblue")
legend("topright",c("training loss","validation loss","early stop"),col=c("red","blue","lightblue"),lty=c(1,1,2))
# dev.off()


vi<-array(0,dim=c(K=3,length(model_bst_both$p_model$Tree_0),4))
for (k in 1:3){
  for (m in 1: length(model_bst_both$p_model$Tree_0)){
    VI<-model_bst_both$p_model$Tree_0[[m]][[k]]$variable.importance
    if (length(VI)>0){
    for (i in 1:length(VI)){
      if (names(VI)[i]=="X1"){
        vi[k,m,1]<-VI[i]
      }
      if (names(VI)[i]=="X2"){
        vi[k,m,2]<-VI[i]
      }
      if (names(VI)[i]=="X3"){
        vi[k,m,3]<-VI[i]
      }
      if (names(VI)[i]=="X4"){
        vi[k,m,4]<-VI[i]
      }
    }
    }
  }
}

summary(model_bst_both$fitted[,c("mu1","mu2","mu3")])
apply(vi[1,,],2,sum)
apply(vi[2,,],2,sum)
apply(vi[3,,],2,sum)

sum(vi[,,1])
sum(vi[,,2])
sum(vi[,,3])
sum(vi[,,4])

# png("./plots/three_gaussians/loss-bst-both-varimp.png")
barplot(c(sum(vi[,,1]),
sum(vi[,,2]),
sum(vi[,,3]),
sum(vi[,,4]))/sum(sum(vi[,,1]),
sum(vi[,,2]),
sum(vi[,,3]),
sum(vi[,,4]))*100,names.arg =  c("x1","x2","x3","x4"),ylab="relative importance",main="relative importance of covariates")
box()
# dev.off()

```

# save results

```{r}
loss_mat <-
  data.frame(
    model = c(
      "homo",
      "glm_mu",
      "glm_p",
      "glm_both",
      "bst_mu",
      "bst_p",
      "bst_both",
      "true"
    ),
    negL = c(
      homo_test_loss,
      glm_test_loss_mu,
      glm_test_loss_p,
      glm_test_loss_both,
      bst_test_loss_mu,
      bst_test_loss_p,
      bst_test_loss_both,
      true_test_loss
    ),
    runT = c(
      time_null[[3]],
      time_glm_mu[[3]],
      time_glm_p[[3]],
      time_glm_b[[3]],
      time_m,
      time_p*60,
      time_b*60,
      0
    )
  )
loss_mat
loss_mat[,2:3]<-round(loss_mat[,2:3],4)
# write.csv(loss_mat,"./plots/three_gaussians/three_gaussian_loss.csv")
```

