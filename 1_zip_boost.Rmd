---
title: "zip boost"
output: html_document
editor_options: 
  chunk_output_type: console
---

# simulated data
```{r}
rm(list = ls())

neg_ll<-function(y,pi0,lambda){
  -ifelse(y==0,log(pi0+(1-pi0)*exp(-lambda)),log(1-pi0)+y*log(lambda)-lambda)
}
p_linear<-function(x) log(x/(1-x))

binomial_p <- function(){
  Family(
    loss = function(y, f) {
      y*f-log(1+exp(f))
    },
    ngradient = function(y, f, w=1){
      y - (exp(f)/(1+exp(f)))
  },
   offset = function(y, w) {
     p = weighted.mean(y, w)
     log(p/(1-p))
   },
  nuisance = function() return(NA),
  response = function(f) exp(f)/(1+exp(f)),
  rclass = function(f) NA,
  name = "binomial_p")
}

n<-10000
set.seed(1)
x1<-rnorm(n,0,0.5)
x2<-runif(n,0,1)
x3<-rgamma(n,2,0.5)
x4<-rbinom(n,1,0.5)
x5<-rbinom(n,1,0.2)

eta<--x1^2 + 0.2*log(x3) - 0.2*x4 +0.2*x4*x1
lambda <- exp(eta)
hist(lambda)
hist(exp(-lambda))
# pi_f<--2*x2^2 +x2 + 0.2*x5+0.3
pi_f<-0
pi0<-exp(pi_f)/(1+exp(pi_f))
mean(pi0)
hist(pi0)
plot(pi0,lambda)

z<-rbinom(n,1,pi0)
y<-(1-z)*rpois(n,lambda = lambda)

dat0<-data.frame(y,x1,x2,x3,x4,x5,lambda,eta,pi0,pi_f,z)
dat0<-dat0[order(dat0$y),]
dat0$ind<-rep(1:5,n/5)
dat_vali<-dat0[dat0$ind==4,]
dat_test<-dat0[dat0$ind==5,]

dat1<-dat0[dat0$ind<5,]
exp(-mean(dat1$y))
sum(dat1$y==0)/length(dat1$y)

(loss_true<-mean(neg_ll(dat_test$y,dat_test$pi0,dat_test$lambda)))
(loss_true_cp<-mean(neg_ll(dat_test$y,mean(dat_test$pi0),dat_test$lambda)))
(loss_true_cl<-mean(neg_ll(dat_test$y,dat_test$pi0,mean(dat_test$lambda))))
(loss_true_cpl<-mean(neg_ll(dat_test$y,mean(dat_test$pi0),mean(dat_test$lambda))))
```

# zip glm

## constant p

```{r}
# initialization of lambda and pi
dat1$lambda_glm<-mean(dat1$y)
dat1$pi_glm<-(sum(dat1$y==0)/length(dat1$y)-exp(-mean(dat1$y)))/(1-exp(-mean(dat1$y)))

lambda_glm_vec<-NULL
pi0_glm_vec<-NULL
loss_glm_vec<-NULL
K<-50

for (k in 1:K){
  # expectation of z
    dat1$z_glm<-ifelse(dat1$y==0,1,0)*dat1$pi_glm/(dat1$pi_glm+(1-dat1$pi_glm)*exp(-dat1$lambda_glm))
  
  # maximization of pi
  glm1<-glm(z_glm ~ 1, family = binomial(link = "logit"), data=dat1)
  dat1$pi_glm<-fitted(glm1)
  
  # maximization of lambda
  glm2<-glm(y ~ x1 + x2 + x3 + x4 + x5, weights = 1-z_glm, family=poisson(link = "log"),data=dat1)
  dat1$lambda_glm<-fitted(glm2)
  
  lambda_glm_vec[k]<-mean(dat1$lambda_glm)
  pi0_glm_vec[k]<-mean(dat1$pi_glm)
  loss_glm_vec[k]<-mean(neg_ll(dat1$y,dat1$pi_glm,dat1$lambda_glm))
}

plot(loss_glm_vec,type="l")

dat_test$pi_glm<-predict(glm1,newdata = dat_test,type = "response")
dat_test$lambda_glm<-predict(glm2,newdata = dat_test,type ="response")

(eGLM_cp_eta<-mean((dat_test$eta-log(dat_test$lambda_glm))^2))
(eGLM_cp_pif<-mean((dat_test$pi_f-p_linear(dat_test$pi_glm))^2))
plot(dat_test$eta,log(dat_test$lambda_glm))
plot(dat_test$pi_f,p_linear(dat_test$pi_glm))

(loss_glm_cp<-mean(neg_ll(dat_test$y,dat_test$pi_glm,dat_test$lambda_glm)))
loss_true
loss_true_cp

exp(-mean(dat_test$y))
sum(dat_test$y==0)/length(dat_test$y)
mean(dat_test$pi_glm)+(1-mean(dat_test$pi_glm))*exp(-mean(dat_test$lambda_glm))
```

## varying p

```{r}
# initialization of lambda and pi
dat1$lambda_glm<-mean(dat1$y)
dat1$pi_glm<-(sum(dat1$y==0)/length(dat1$y)-exp(-mean(dat1$y)))/(1-exp(-mean(dat1$y)))

lambda_glm_vec<-NULL
pi0_glm_vec<-NULL
loss_glm_vec<-NULL
K<-50

for (k in 1:K){
  # expectation of z
    dat1$z_glm<-ifelse(dat1$y==0,1,0)*dat1$pi_glm/(dat1$pi_glm+(1-dat1$pi_glm)*exp(-dat1$lambda_glm))
  
  # maximization of pi
  glm1<-glm(z_glm ~ x1 + x2 + x3 + x4 + x5, family = binomial(link = "logit"), data=dat1)
  dat1$pi_glm<-fitted(glm1)
  
  # maximization of lambda
  glm2<-glm(y ~ x1 + x2 + x3 + x4 + x5, weights = 1-z_glm, family=poisson(link = "log"),data=dat1)
  dat1$lambda_glm<-fitted(glm2)
  
  lambda_glm_vec[k]<-mean(dat1$lambda_glm)
  pi0_glm_vec[k]<-mean(dat1$pi_glm)
  loss_glm_vec[k]<-mean(neg_ll(dat1$y,dat1$pi_glm,dat1$lambda_glm))
}

plot(loss_glm_vec,type="l")

dat_test$pi_glm<-predict(glm1,newdata = dat_test,type = "response")
dat_test$lambda_glm<-predict(glm2,newdata = dat_test,type ="response")

plot(dat_test$eta,log(dat_test$lambda_glm))
plot(dat_test$pi_f,p_linear(dat_test$pi_glm))

(eGLM_vp_eta<-mean((dat_test$eta-log(dat_test$lambda_glm))^2))
(eGLM_vp_pif<-mean((dat_test$pi_f-p_linear(dat_test$pi_glm))^2))
eGLM_cp_eta
eGLM_cp_pif
(loss_glm_vp<-mean(neg_ll(dat_test$y,dat_test$pi_glm,dat_test$lambda_glm)))
loss_glm_cp
loss_true
loss_true_cp

exp(-mean(dat_test$y))
sum(dat_test$y==0)/length(dat_test$y)
mean(dat_test$pi_glm)+(1-mean(dat_test$pi_glm))*exp(-mean(dat_test$lambda_glm))
```

# zip boost

## constant p

```{r}
library("mboost")
library("rpart")
library("rpart.plot")
dat1<-dat1[dat1$ind<4,]

# initialization
dat1$lambda_boost<-dat1$lambda_glm
dat1$pi_boost<-dat1$pi_glm

lambda_boost_vec<-NULL
pi0_boost_vec<-NULL
loss_boost_vec<-NULL
loss_boost_vali<-NULL
loss_boost_test<-NULL
Mstop<-300
nu<-0.1
K<-3

for (k in 1:K){
  
  # expectation of z
  dat1$z_boost <-
    ifelse(dat1$y == 0, 1, 0) * dat1$pi_boost / (dat1$pi_boost + (1 - dat1$pi_boost) * exp(-dat1$lambda_boost))
  
  # maximization of p
  dat1$pi_boost<-mean(dat1$z_boost)
  dat_vali$pi_boost<-mean(dat1$z_boost)
  dat_test$pi_boost<-mean(dat1$z_boost)
  
  # maximization of lambda
  boost2 <-
    blackboost(
      y ~ x1 + x2 + x3 + x4 + x5,
      weights = 1 - z_boost,
      family = Poisson(),
      data = dat1,
      control = boost_control(mstop = Mstop, nu = nu))
  #,
   #   tree_controls = partykit::ctree_control(maxdepth = 2)
    #)
  dat1$lambda_boost <- fitted(boost2,type="response")
  dat_vali$lambda_boost <- predict(boost2,type="response", newdata = dat_vali)
  dat_test$lambda_boost <- predict(boost2,type="response", newdata = dat_test)

  lambda_boost_vec[k] <- mean(dat1$lambda_boost)
  pi0_boost_vec[k] <- mean(dat1$pi_boost)
  loss_boost_vec[k] <-
    mean(neg_ll(dat1$y, dat1$pi_boost, dat1$lambda_boost))
  loss_boost_test[k]<-
    mean(neg_ll(dat_test$y, dat_test$pi_boost, dat_test$lambda_boost))
  loss_boost_vali[k]<-
    mean(neg_ll(dat_vali$y, dat_vali$pi_boost, dat_vali$lambda_boost))
}

matplot(cbind(loss_boost_vec,loss_boost_vali,loss_boost_test),lty=c(1,1,1),col=c("red","gray","blue"),type="l")
plot(dat_test$eta,log(dat_test$lambda_boost))
plot(dat_test$pi_f,p_linear(dat_test$pi_boost))

(eBST_cp_eta<-mean((dat_test$eta-log(dat_test$lambda_boost))^2))
(eBST_cp_pif<-mean((dat_test$pi_f-p_linear(dat_test$pi_boost))^2))
eGLM_vp_eta
eGLM_vp_pif
eGLM_cp_eta
eGLM_cp_pif
(loss_BST_cp<-mean(neg_ll(dat_test$y,dat_test$pi_boost,dat_test$lambda_boost)))
loss_glm_vp
loss_glm_cp
loss_true_cp

exp(-mean(dat_test$y))
sum(dat_test$y==0)/length(dat_test$y)
mean(dat_test$pi_boost)+(1-mean(dat_test$pi_boost))*exp(-mean(dat_test$lambda_boost))
```

## varying p

```{r}
library("mboost")
library("rpart")
library("rpart.plot")

# initialization
dat1$lambda_boost<-dat1$lambda_glm
dat1$pi_boost<-dat1$pi_glm

lambda_boost_vec<-NULL
pi0_boost_vec<-NULL
loss_boost_vec<-NULL
loss_boost_vali<-NULL
loss_boost_test<-NULL
Mstop_lambda<-300
Mstop_p<-100

nu<-0.1
K<-2

for (k in 1:K){
  
  # expectation of z
  dat1$z_boost <-
    ifelse(dat1$y == 0, 1, 0) * dat1$pi_boost / (dat1$pi_boost + (1 - dat1$pi_boost) * exp(-dat1$lambda_boost))
  
  # maximization of p
  boost1 <-
    blackboost(
      z_boost ~ x1 + x2 + x3 + x4 + x5,
      family = binomial_p(),
      data = dat1,
      control = boost_control(mstop = Mstop_p, nu = nu))
  
  dat1$pi_boost<-fitted(boost1,type="response")
  dat_vali$pi_boost<-predict(boost1,newdata=dat_vali,type="response")
  dat_test$pi_boost<-predict(boost1,newdata=dat_test,type="response")
  
  # maximization of lambda
  boost2 <-
    blackboost(
      y ~ x1 + x2 + x3 + x4 + x5,
      weights = 1 - z_boost,
      family = Poisson(),
      data = dat1,
      control = boost_control(mstop = Mstop_lambda, nu = nu))
  
  dat1$lambda_boost <- fitted(boost2,type="response")
  dat_vali$lambda_boost <- predict(boost2,type="response", newdata = dat_vali)
  dat_test$lambda_boost <- predict(boost2,type="response", newdata = dat_test)

  lambda_boost_vec[k] <- mean(dat1$lambda_boost)
  pi0_boost_vec[k] <- mean(dat1$pi_boost)
  loss_boost_vec[k] <-
    mean(neg_ll(dat1$y, dat1$pi_boost, dat1$lambda_boost))
  loss_boost_test[k]<-
    mean(neg_ll(dat_test$y, dat_test$pi_boost, dat_test$lambda_boost))
  loss_boost_vali[k]<-
    mean(neg_ll(dat_vali$y, dat_vali$pi_boost, dat_vali$lambda_boost))
}

matplot(cbind(loss_boost_vec,loss_boost_vali,loss_boost_test),lty=c(1,1,1),col=c("red","gray","blue"),type="l")
plot(dat_test$eta,log(dat_test$lambda_boost))
plot(dat_test$pi_f,p_linear(dat_test$pi_boost))

(eBST_vp_eta<-mean((dat_test$eta-log(dat_test$lambda_boost))^2))
(eBST_vp_pif<-mean((dat_test$pi_f-p_linear(dat_test$pi_boost))^2))
eBST_cp_eta
eBST_cp_pif
eGLM_vp_eta
eGLM_vp_pif
eGLM_cp_eta
eGLM_cp_pif
(loss_BST_vp<-mean(neg_ll(dat_test$y,dat_test$pi_boost,dat_test$lambda_boost)))
loss_BST_cp
loss_glm_vp
loss_glm_cp
loss_true_cp

exp(-mean(dat_test$y))
sum(dat_test$y==0)/length(dat_test$y)
mean(dat_test$pi_boost)+(1-mean(dat_test$pi_boost))*exp(-mean(dat_test$lambda_boost))
```

# comparison

```{r}
loss_mat<-data.frame(model=c("glm_cp","glm_vp","bst_cp","bst_vp","true"),negL=c(loss_glm_cp,loss_glm_vp,loss_BST_cp,loss_BST_vp,loss_true),error_eta=NA, error_pif=NA)
loss_mat$error_eta<-c(eGLM_cp_eta,eGLM_vp_eta,eBST_cp_eta,eBST_vp_eta,NA)
loss_mat$error_pif<-c(eGLM_cp_pif,eGLM_vp_pif,eBST_cp_pif,eBST_vp_pif,NA)
loss_mat
# write.csv(loss_mat,"./plots/zip/zip_loss_mat.csv")
```
